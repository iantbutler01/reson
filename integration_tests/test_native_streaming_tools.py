#!/usr/bin/env python3
"""Test streaming native tool calls."""

import asyncio
import os
from typing import AsyncGenerator
from reson import agentic_generator, Runtime
from reson.types import Deserializable


class CalculationQuery(Deserializable):
    """A calculation with two numbers."""

    a: int
    b: int
    operation: str = "add"


def calculate(query: CalculationQuery) -> int:
    """Perform a calculation."""
    if query.operation == "add":
        return query.a + query.b
    elif query.operation == "multiply":
        return query.a * query.b
    else:
        return 0


def get_info(topic: str) -> str:
    """Get information about a topic."""
    return (
        f"Information about {topic}: This is a comprehensive overview of the subject."
    )


async def test_streaming_native_tools():
    """Test streaming with native tool calls."""
    print("ğŸ§ª Testing Streaming Native Tool Calls")

    @agentic_generator(model="openrouter:anthropic/claude-sonnet-4", native_tools=True)
    async def streaming_agent(
        query: str, runtime: Runtime
    ) -> AsyncGenerator[str, None]:
        runtime.tool(calculate)
        runtime.tool(get_info)

        # Start streaming
        async for chunk in runtime.run_stream(prompt=query):
            # Handle different chunk types based on the actual streaming interface
            if isinstance(chunk, tuple) and len(chunk) == 2:
                chunk_type, chunk_content = chunk
                if chunk_type == "reasoning":
                    yield f"ğŸ§  Thinking: {chunk_content}"
                elif chunk_type == "content":
                    yield f"ğŸ“ Content: {chunk_content}"
                elif chunk_type == "tool_call_complete":
                    yield f"ğŸ”§ Tool Call: {chunk_content._tool_name if hasattr(chunk_content, '_tool_name') else chunk_content}"
                else:
                    yield f"ğŸ”§ Stream: {chunk_type} - {chunk_content}"
            else:
                # Single chunk (traditional streaming)
                yield f"ğŸ“ Chunk: {chunk}"

    try:
        print("Starting streaming test...")
        chunks = []

        async for chunk in streaming_agent(
            "Calculate 15 + 27 and then get info about 'machine learning'"
        ):
            print(chunk)
            chunks.append(chunk)

        print(f"\nâœ… Streaming test completed with {len(chunks)} chunks")
        return True

    except Exception as e:
        print(f"âŒ Streaming test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_google_streaming_tool_detection():
    """Test Google streaming tool call detection."""
    print("\nğŸ§ª Testing Google Streaming Tool Detection")

    @agentic_generator(model="vertex-gemini:gemini-2.5-flash", native_tools=True)
    async def vertex_streaming_agent(
        query: str, runtime: Runtime
    ) -> AsyncGenerator[str, None]:
        runtime.tool(calculate)
        runtime.tool(get_info)

        # Start streaming
        async for chunk in runtime.run_stream(prompt=query):
            if isinstance(chunk, tuple) and len(chunk) == 2:
                chunk_type, chunk_content = chunk
                if chunk_type == "reasoning":
                    yield f"ğŸ§  Vertex Thinking: {chunk_content}"
                elif chunk_type == "content":
                    yield f"ğŸ“ Vertex Content: {chunk_content}"
                elif chunk_type == "tool_call_complete":
                    yield f"ğŸ”§ Vertex Buffered Tool Call: {chunk_content._tool_name if hasattr(chunk_content, '_tool_name') else 'detected'}"
                else:
                    yield f"ğŸ”§ Vertex Stream: {chunk_type}"
            else:
                yield f"ğŸ“ Vertex Chunk: {chunk}"

    try:
        print("Starting Google streaming test...")
        chunks = []

        async for chunk in vertex_streaming_agent(
            "Calculate 8 + 12 using the calculate tool"
        ):
            print(chunk)
            chunks.append(chunk)

        print(f"\nâœ… Google streaming test completed with {len(chunks)} chunks")
        return True

    except Exception as e:
        print(f"âŒ Google streaming test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_google_anthropic_streaming_tools():
    """Test Google Anthropic streaming tool calls."""
    print("\nğŸ§ª Testing Google Anthropic Streaming Tool Calls")

    @agentic_generator(model="google-anthropic:claude-3-5-sonnet", native_tools=True)
    async def google_anthropic_streaming_agent(
        query: str, runtime: Runtime
    ) -> AsyncGenerator[str, None]:
        runtime.tool(calculate)
        runtime.tool(get_info)

        # Start streaming
        async for chunk in runtime.run_stream(prompt=query):
            if isinstance(chunk, tuple) and len(chunk) == 2:
                chunk_type, chunk_content = chunk
                if chunk_type == "reasoning":
                    yield f"ğŸ§  GA Thinking: {chunk_content}"
                elif chunk_type == "content":
                    yield f"ğŸ“ GA Content: {chunk_content}"
                elif chunk_type == "tool_call_complete":
                    yield f"ğŸ”§ GA Tool Call: {chunk_content._tool_name if hasattr(chunk_content, '_tool_name') else 'detected'}"
                else:
                    yield f"ğŸ”§ GA Stream: {chunk_type}"
            else:
                yield f"ğŸ“ GA Chunk: {chunk}"

    try:
        print("Starting Google Anthropic streaming test...")
        chunks = []

        async for chunk in google_anthropic_streaming_agent(
            "Use calculate to multiply 6 by 7"
        ):
            print(chunk)
            chunks.append(chunk)

        print(
            f"\nâœ… Google Anthropic streaming test completed with {len(chunks)} chunks"
        )
        return True

    except Exception as e:
        print(f"âŒ Google Anthropic streaming test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_streaming_tool_execution():
    """Test streaming with actual tool execution."""
    print("\nğŸ§ª Testing Streaming with Tool Execution")

    @agentic_generator(model="openrouter:anthropic/claude-sonnet-4", native_tools=True)
    async def execution_agent(
        query: str, runtime: Runtime
    ) -> AsyncGenerator[str, None]:
        runtime.tool(calculate)

        history = []

        # Stream initial response
        yield "ğŸš€ Starting calculation..."

        async for chunk_type, chunk in runtime.run_stream(prompt=query):
            if chunk_type == "tool_call_complete":
                # We got a complete tool call, let's execute it
                yield f"ğŸ”§ Executing tool: {chunk}"

                # Convert streaming chunk to tool call object (simplified)
                # In real implementation, this would be handled by the system
                break
            else:
                yield f"ğŸ“¡ Streaming: {chunk_type} - {chunk}"

        # Simulate getting the tool call result and continuing
        yield "âœ… Tool execution completed"

    try:
        chunks = []
        async for chunk in execution_agent("Calculate 25 + 17"):
            print(chunk)
            chunks.append(chunk)

        print(f"\nâœ… Execution streaming test completed with {len(chunks)} chunks")
        return True

    except Exception as e:
        print(f"âŒ Execution streaming test failed: {e}")
        return False


async def main():
    """Run streaming native tool tests."""
    print("ğŸš€ Testing Native Tool Streaming")
    print("=" * 50)

    if not os.getenv("OPENROUTER_API_KEY"):
        print("âŒ OPENROUTER_API_KEY not set")
        return

    print("âœ… API key configured")

    tests = [
        test_streaming_native_tools(),
        test_google_streaming_tool_detection(),
        test_google_anthropic_streaming_tools(),
        test_streaming_tool_execution(),
    ]

    results = await asyncio.gather(*tests, return_exceptions=True)
    success_count = sum(1 for r in results if r is True)

    print(f"\nğŸ“Š Streaming Test Results: {success_count}/{len(results)} passed")

    if success_count == len(results):
        print("ğŸ‰ All streaming tests passed!")
    else:
        print("âš ï¸  Some streaming tests failed.")


if __name__ == "__main__":
    asyncio.run(main())
