import datetime
import functools
import pathlib
import tomllib
from asimov.asimov_base import AsimovBase
import enum
import re
import uuid
import aiohttp
from bs4 import BeautifulSoup, NavigableString
from urllib.parse import urlparse, parse_qs
import asyncio

from daneel.data.bismuth_config import BismuthTOML, BismuthTestTOML
from daneel.data.postgres.models import (
    ChatSessionEntity,
    FeatureEntity,
    ProjectEntity,
    OrganizationEntity,
    GitHubAppInstallEntity,
    GitLabAppInstallEntity,
    LinearAppInstallEntity,
    AtlassianAppInstallEntity,
)
from daneel.data.file_rpc import FileRPC
from daneel.data.graph_rag import GraphRag
from daneel.data.graph_rag.hybrid_search import CitationEntity, ExternalContextEntity

from daneel.executors.aci.bug_finding_visualizer import BugFindingVisualizer
from daneel.executors.aci.fuzz_visualizer import FuzzVisualizer
from daneel.executors.aci.aci_interactive_driver_executor import ACIDriverExecutor
from daneel.executors.aci.visualization import ACIVisualizer
import json
import logging
import gitlab
from gql import gql, Client as GQLClient
from gql.transport.aiohttp import (
    AIOHTTPTransport as GQLAIOHTTPTransport,
)
import httpx

from asimov.graph import ModuleConfig
from daneel.executors.aci.aci_system_analysis_executor import (
    ACISystemAnalysisExecutor,
)
from daneel.data.graph_rag.graph import KGNodeType
from daneel.executors.aci.prompts import *
from daneel.executors.summary_executor import SummaryExecutor

from asyncio import Semaphore

from asimov.graph import AgentModule, ModuleType
from asimov.services.inference_clients import InferenceClient
from difflib import SequenceMatcher

import os
from daneel.services.analysis_client import (
    CodeAnalysisClient,
)
import daneel.api.services.github as github_service
from daneel.api.services.gitlab import expand_gitlab_references
from daneel.api.services.jira import expand_jira_references
from daneel.utils.adf_to_md import adf_to_md

from pydantic import Field, model_validator, PrivateAttr
from jinja2 import Template

from typing import Any, Awaitable, Callable, Iterable, Optional, cast
from asimov.caches.cache import Cache
import textwrap

from daneel.utils import find_text_chunk
from daneel.services.ast_code_analysis.analysis.source_file import (
    SourceFile,
    UnknownExtensionException,
)
from daneel.services.ast_code_analysis import repo_skeleton, Repository
import math

from daneel.utils.repo import get_clone_url
from daneel.utils.tracing import trace_output
from daneel.utils.websockets import (
    ACIMessage,
    ChatModifiedFile,
    RunCommandMessage,
    RunCommandResponse,
    WSMessage,
    WSMessageType,
    null_recv_callback,
    null_send_callback,
)

from daneel.executors.aci.aci_types import *
from daneel.constants import USE_GASP_TOOLS

LINES_IN_VIEW = 500
LINES_IN_VIEW_CONSTRAINED = 2000
RECURSION_LIMIT = 1

GIT_HOST = os.environ.get("GIT_HOST", "localhost:8765")


def is_url(path: str) -> bool:
    """Checks if the given path is a URL."""
    return re.match(r"^(https|http)://", path) is not None


class ACIExecutionMode(enum.Enum):
    SINGLE = "single"
    MULTI = "multi"


class ACI(AsimovBase):
    cache: Cache
    viewer_state: str = Field(default="")
    tool_executors: dict["str", AgentModule] = Field(default_factory=dict)
    send_message_callback: Callable[[WSMessage], Awaitable[None]]
    recv_message_callback: Callable[[], Awaitable[WSMessage]]
    file_rpc: FileRPC
    initial_turns: int = 25
    interactive_mode: bool = Field(default=False)
    run_tests_on_finalize: bool = Field(default=False)
    recursion_depth: int = Field(default=0)
    finalized: bool = Field(default=False)
    mode: ACIExecutionMode = Field(default=ACIExecutionMode.MULTI)
    _input_task: str = PrivateAttr()
    _step_count: int = PrivateAttr()
    _pinned_files: dict[str, str] = PrivateAttr(default_factory=dict)
    _turns_remaining: int = PrivateAttr(default=0)
    _recursive_task: Optional[str] = PrivateAttr(default=None)
    _attempted_finalize: bool = PrivateAttr(default=False)
    _last_shown_viewer_state: int = PrivateAttr(
        default=0
    )  # Turn number that the full view state was last shown
    _mode: ACIMode = PrivateAttr(default=ACIMode.DRIVER)
    _humanlayer_enabled: bool = PrivateAttr(default=False)
    _logger: logging.Logger = PrivateAttr()
    _url_content_cache: dict[str, str] = PrivateAttr(
        default_factory=dict
    )  # Simple in-memory cache for URL content for this ACI instance
    _question_out_queue: asyncio.Queue = PrivateAttr(default_factory=asyncio.Queue)
    _apply_bug_fixes: bool = PrivateAttr(default=False)

    tool_schemas: dict[str, dict[str, Any]] = {
        "switch_file": {
            "name": "switch_file",
            "description": "Close the specific file to viewing and editing when you are finished making changes to it. Only close the file if you're sure you are done with the file such as after you've made all your changes.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_id": {
                        "type": "number",
                        "description": "The id of the file you want to switch to as shown next to the file path between the <files> tags.",
                    }
                },
                "required": ["file_id"],
            },
        },
        "switch_to_navigation_mode": {
            "name": "switch_to_navigation_mode",
            "description": "Switch to navigation mode, this mode unlocks the abilty to scroll a file, switch active files, go to def, go to line, list files, find by phrase, close a file and open a file.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "goal": {
                        "type": "string",
                        "description": "What you want the navigation system to accomplish before returning control to you. Be detailed, it should be step by step.",
                    }
                },
                "required": ["goal"],
            },
        },
        "switch_to_editing_mode": {
            "name": "switch_to_editing_mode",
            "description": "Switch to editing mode, this mode unlocks the ability to delete a file, create a file, edit existing files, run commands and testing / building.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "goal": {
                        "type": "string",
                        "description": "What you want the editing system to accomplish before returning control to you. Be detailed, it should be step by step.",
                    }
                },
                "required": ["goal"],
            },
        },
        "show_skeleton": {
            "name": "show_repo_skeleton",
            "description": "Show a 'skeleton' of the files you provide from the codebase, which is the source files with only the class and function declarations/prototypes. In the case of python, this is similar to a type stub file for example. This is useful for understanding the structure of files in the codebase and the relationships between files.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "interesting_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "A list of interesting files whose skeletons will be shown.",
                    }
                },
                "required": [],
            },
        },
        "switch_to_driver_mode": {
            "name": "switch_to_driver_mode",
            "description": "Switch to driver mode which can call finalize if the task is done or switch to one of 3 other modes, navigation, editing or debug modes depending on what it decides based on the current visualizer state.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "results": {
                        "type": "string",
                        "description": "Thoughts about the results of your current goal that you want to communicate to the driver system as you return control.",
                    }
                },
                "required": ["results"],
            },
        },
        "analyze_system": {
            "name": "analyze_system",
            "description": "Performs a deep analysis of recent issues you are seeing in completing your task and attempts to provide tactical steps to become unstuck and complete the task.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "focus": {
                        "type": "string",
                        "description": "The specific aspect or issue you want to analyze. This helps direct the analysis to relevant parts of the system.",
                    }
                },
                "required": ["focus"],
            },
        },
        "create_file": {
            "name": "create_file",
            "description": "Creates a new file with the contents you specify and switches to the file in the viewer.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "thoughts": {
                        "type": "string",
                        "description": "Your thoughts about the change you are making given the state of the system. These should detail why this change is moving you closer to completing the task as stated in the users prompt.",
                    },
                    "step": {
                        "type": "string",
                        "description": "An english description of the change you are making. This helps document the purpose of the file creation.",
                    },
                    "file": {
                        "type": "string",
                        "description": "The name of the file you are creating.",
                    },
                    "content": {
                        "type": "string",
                        "description": "The contents that will be written to the file.",
                    },
                },
                "required": [
                    "thoughts",
                    "file",
                    "step",
                    "content",
                ],
            },
        },
        "edit_file": {
            "name": "edit_file",
            "description": "Performs a targeted replacement of specified text within a file, for lines in the current viewer state. This operation allows you to identify specific lines of text and replace them with new content while maintaining the file's structure. Each edit is tracked with a unique identifier and includes a human-readable description of the change being made. This operation is useful for making precise modifications to configuration files, source code, or any text-based document where specific lines need to be updated.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "thoughts": {
                        "type": "string",
                        "description": "Your thoughts about the change you are making given the state of the system. These should detail why this change is moving you closer to completing the task as stated in the users prompt.",
                    },
                    "file_id": {
                        "type": "number",
                        "description": "The id of the file you want to switch to as shown next to the file path between the <files> tags.",
                    },
                    "step": {
                        "type": "string",
                        "description": "An english description of the change you are making. This helps document the purpose of the edit.",
                    },
                    "file": {
                        "type": "string",
                        "description": "The name of the file you are editing.",
                    },
                    "lines_to_replace": {
                        "type": "string",
                        "description": "The exact content of the lines of text to be replaced. These lines must exist within the content currently in the viewer state. Whitespace and linebreaks must be the same. Do not include the line number.",
                    },
                    "replace_text": {
                        "type": "string",
                        "description": "The content of the lines of text doing the replacing. This field is absolutely required and contains the new content that will replace the specified lines. Do not include the line number.",
                    },
                    "id": {
                        "type": "string",
                        "description": "A unique id representing the edit. This allows for tracking and referencing specific changes.",
                    },
                },
                "required": [
                    "thoughts",
                    "file",
                    "lines_to_replace",
                    "step",
                    "replace_text",
                    "id",
                    "file_id",
                ],
            },
        },
        "close_file": {
            "name": "close_file",
            "description": "Closes a currently open file in the editing session. When the last open file is closed, the editing process automatically terminates. This operation helps manage system resources and maintain a clean workspace by closing files that are no longer needed for the current editing task.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_id": {
                        "type": "number",
                        "description": "The numeric identifier of the file to close, as displayed next to the file path within the <files> tags. Each open file has a unique ID that persists throughout the editing session.",
                    }
                },
                "required": ["file_id"],
            },
        },
        "delete_file": {
            "name": "delete_file",
            "description": "Deletes the currently open file from the project. This action is permanent, you must recreate the file if you wish to work on it again.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "step": {
                        "type": "string",
                        "description": "An english description of the change you are making. This helps document the purpose of the edit.",
                    },
                    "file_id": {
                        "type": "number",
                        "description": "The numeric identifier of the file to delete, as displayed next to the file path within the <files> tags. Each open file has a unique ID that persists throughout the editing session.",
                    },
                },
                "required": ["file_id", "step"],
            },
        },
        "read_file": {
            "name": "read_file",
            "description": "Read the content of a file or URL, and show its entire contents in the viewer. If the file does not exist, an error will be explicitly returned. All other cases are treated as success.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file": {
                        "type": "string",
                        "description": "The path to the file you wish to read, these can be determined through the 'list_files' command. You can also pass a URL to open a web page.",
                    }
                },
                "required": ["file"],
            },
        },
        "read_file_diff": {
            "name": "read_file_diff",
            "description": "Read the content of a file, showing any changes from the PR inline. If the file does not exist, an error will be explicitly returned. All other cases are treated as success.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file": {
                        "type": "string",
                        "description": "The path to the file you wish to read, these can be determined through the 'list_files' command.",
                    }
                },
                "required": ["file"],
            },
        },
        "list_files": {
            "name": "list_files",
            "description": "Lists all available files in the project, this will populate the system analysis portion of the viewer with a list of all available files in the project.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "subpath": {
                        "type": "string",
                        "description": "The directory subpath you want to list files under.",
                    }
                },
                "required": [],
            },
        },
        "semantic_search": {
            "name": "semantic_search",
            "description": "Searches for symbols (function or class) within the codebase based on a natural language query. This tool uses callgraph information and can find dependent symbols as well as leaf symbols. This operation helps locate relevant code components by identifying symbols related to the query up and down call graphs. It's particularly useful for understanding code dependencies and exploring the implementation details of referenced symbols especially when combined with 'go_to_def'. Quality of this tool is language dependent. If you are not getting the results you expect, try rephrasing your query or using 'find_phrase' instead.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query should describe the symbol you are looking for in plain English based on the functionality or purpose of the symbol. For example, 'function that reads a file' or 'class that handles user authentication'.",
                    }
                },
                "required": ["query"],
            },
        },
        "find_phrase": {
            "name": "find_phrase",
            "description": "Finds the specific, case sensitive phrase across the codebase, returns filename and line number for all matches.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "exact_match_phrase": {
                        "type": "string",
                        "description": "A phrase to search for exact case sensitive matches over the codebase.",
                    }
                },
                "required": ["exact_match_phrase"],
            },
        },
        "go_to_line": {
            "name": "go_to_line",
            "description": "Jumps to the specific line number in the current active file. Useful for navigating the codebase combined with 'find' and 'open'. This call will make sure the line is in the current viewer state. Calling it more than once does nothing.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "line_num": {
                        "type": "number",
                        "description": "The line to jump to in the current active file.",
                    }
                },
                "required": ["line_num"],
            },
        },
        "run_command": {
            "name": "run_command",
            "description": "Run the specified bash command and return the result. Each command is executed in a separate shell session, so commands that rely on environment variables or other state changes will not persist between commands.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "The bash command to run.",
                    }
                },
                "required": ["command"],
            },
        },
        "recurse": {
            "name": "recurse",
            "description": "A subagent will attempt to independently finish the singular specifc subtask you specify for it. This spawns a full copy of yourself with all current state included. This is a very powerful command and should be used only in cases that truly call for it such when you are very stuck or the task is vauge or broad.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "subtask": {
                        "type": "string",
                        "description": "A SINGLE specific and detailed subtask of the larger task you are trying to complete that you believe would be best served with having a copy of your full attention on it. Vague subtasks are likely to fail, so be very specific in what you ask.",
                    }
                },
                "required": ["subtask"],
            },
        },
        "recurse_fuzz": {
            "name": "recurse_fuzz",
            "description": "A subagent will attempt to test the specified target using fuzzing techniques. This spawns a full copy of yourself with all current state included.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "target": {
                        "type": "string",
                        "description": "The target function to fuzz test. The function should be pure (not depend on file I/O, network I/O, etc.) and have a well-defined input/output contract.",
                    }
                },
                "required": ["target"],
            },
        },
        "scroll_down_file": {
            "name": "scroll_down_file",
            "description": "Navigates downward in the currently active file to reveal additional content. This operation is essential for reviewing or analyzing files that are too long to display in a single view. It enables systematic exploration of file contents by moving the viewport forward through the document.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "scroll": {
                        "type": "number",
                        "description": "The number of lines to move downward in the file. Must be a positive integer. Larger values will reveal more content at once, while smaller values allow for more precise navigation.",
                    }
                },
                "required": ["scroll"],
            },
        },
        "scroll_up_file": {
            "name": "scroll_up_file",
            "description": "Navigates upward in the currently active file to reveal previous content. This operation allows for reviewing earlier portions of the file that have scrolled out of view. It's particularly useful when needing to reference or compare content across different sections of the file.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "scroll": {
                        "type": "number",
                        "description": "The number of lines to move upward in the file. Must be a positive integer. Larger values will reveal more previous content at once, while smaller values enable fine-grained navigation.",
                    }
                },
                "required": ["scroll"],
            },
        },
        "go_to_def": {
            "name": "go_to_def",
            "description": "Navigates directly to the definition of a specified symbol (function or class) anywhere within the codebase. This operation enables quick navigation between related code components, automatically opening new files if needed. It's particularly useful for understanding code dependencies and exploring the implementation details of referenced symbols.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "symbol": {
                        "type": "string",
                        "description": "The name of the function or class whose definition you want to navigate to.",
                    },
                    "line": {
                        "type": "string",
                        "description": "The line number of the current file which references the symbol.",
                    },
                },
                "required": ["symbol", "line"],
            },
        },
        "find_references": {
            "name": "find_references",
            "description": "Finds all references to a specified symbol (function or class) within the codebase. This operation provides a comprehensive list of all locations where the symbol is used, enabling you to explore the context and usage of the symbol across the codebase.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "symbol": {
                        "type": "string",
                        "description": "The name of the function or class whose references you want to find.",
                    },
                    "line": {
                        "type": "string",
                        "description": "The line number of the current file which references the symbol.",
                    },
                },
                "required": ["symbol", "line"],
            },
        },
        "reach_out_to_human_for_assistance": {
            "name": "reach_out_to_human_for_assistance",
            "description": "Reach out to humans for assistance when you are stuck writing code that means tests or building are getting stuck failing.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "message": {
                        "type": "string",
                        "description": "The message you want to send to the human reaching out for help.",
                    }
                },
                "required": ["message"],
            },
        },
        "report_bug_ci": {
            "name": "report_bug",
            "description": "Report a bug to the user as part of code review.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file": {
                        "type": "string",
                        "description": "The file that contains the bug.",
                    },
                    "lines_to_replace": {
                        "type": "string",
                        "description": "The exact content of the lines of text to be replaced. These lines must exist within the content currently in the viewer state. Whitespace and linebreaks must be the same. Do not include the line number.",
                    },
                    "replace_text": {
                        "type": "string",
                        "description": "The new content that will replace the specified lines. This field is required if a fix for the problem is easily implemented. If a bug report is complex or only advisory, do not include this field. Do not include the line number.",
                    },
                    "bug_description": {
                        "type": "string",
                        "description": "A detailed description of the bug you are experiencing. Use markdown formatting to make the bug report clear and easy to understand, and cite sources with the [source](URL) format.",
                    },
                    "grounding": {
                        "type": "string",
                        "description": 'What makes you believe the bug is truly an issue. This should be the name of a failing test, or the word "fuzzing" if the bug was found through fuzzing. Omit this field if no test or fuzzing was involved.',
                    },
                },
                "required": [
                    "file",
                    "lines_to_replace",
                    "replace_text",
                    "bug_description",
                ],
            },
        },
        "pop_go_to_def": {
            "name": "pop_go_to_def",
            "description": "Pop the last item off the call stack and return to the previous file and line.",
            "input_schema": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        },
        "add_question": {
            "name": "add_question",
            "description": "Add one or more questions to guide further exploration.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "questions": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "The questions you want answered based on your current understanding of the codebase. You must include the path of the file you're looking at in each question so another system looking at these questions can locate the relevant code",
                    }
                },
                "required": ["questions"],
            },
        },
        "resolve_question": {
            "name": "resolve_question",
            "description": "Resolve a question previously asked to provider better understanding of the codebase.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "question_id": {
                        "type": "number",
                        "description": "The id of the question you want to resolve.",
                    },
                    "answer": {
                        "type": "string",
                        "description": "The answer to the question you want to resolve.",
                    },
                },
                "required": ["question_id", "answer"],
            },
        },
        "add_memory": {
            "name": "add_memory",
            "description": "Add a memory about a non-obvious fact that you have found while exploring.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "memory": {
                        "type": "string",
                        "description": "The memory you want to add. Be sure to include all relevant information to help you remember the fact later.",
                    }
                },
                "required": ["memory"],
            },
        },
        "run_fuzzer": {
            "name": "run_fuzzer",
            "description": "Run the given fuzzer against the code base.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The code for the fuzzer.",
                    },
                    "filename": {
                        "type": "string",
                        "description": "The repo-relative filename the fuzzer code will be written to.",
                    },
                },
                "required": ["code", "filename"],
            },
        },
        "search_docs": {
            "name": "search_docs",
            "description": "Searches the documentation for the given query and returns the results.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query to search for.",
                    },
                },
                "required": ["query"],
            },
        },
        "finalize": {
            "name": "finalize",
            "description": "Mark the task you're working on as done. Only use this when you're certain your work is finished as it can complete the process.",
            "input_schema": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        },
        "think": {
            "name": "think",
            "description": "Freely describe and reflect on what you know so far, things that you tried, and how that aligns with your objective and the user's intent. You can play through different scenarios, weigh options, and reason about possible next next steps. The user will not see any of your thoughts here, so you can think freely.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "thoughts": {"type": "array", "items": {"type": "string"}}
                },
                "required": ["thoughts"],
            },
        },
    }

    analysis_endpoint: str = os.environ.get("CODE_ANALYSIS_URL", "localhost:8051")
    _analysis_client: CodeAnalysisClient = PrivateAttr()

    @model_validator(mode="after")
    def setup_analysis_client(self):
        self._analysis_client = CodeAnalysisClient(self.analysis_endpoint)
        return self

    @classmethod
    async def _create_executors(
        cls,
        inference_client: InferenceClient,
        inference_client_factory: Callable[[str], InferenceClient],
        send_message_callback: Callable[[WSMessage], Awaitable[None]],
        recv_message_callback: Callable[[], Awaitable[WSMessage]],
        aci: "ACI",
    ) -> dict[str, AgentModule]:
        from daneel.executors.bug_detection.fuzz_gen import FuzzGenExecutor

        return {
            "analyze_system": ACISystemAnalysisExecutor(
                name="ACISystemAnalysis",
                type=ModuleType.EXECUTOR,
                inference_client=inference_client,
            ),
            "recurse": ACIDriverExecutor(
                name=f"RecursiveACIDriverExecutor",
                type=ModuleType.EXECUTOR,
                config=ModuleConfig(
                    timeout=3000,
                ),
                inference_client_factory=inference_client_factory,
                send_message_callback=send_message_callback,
                recv_message_callback=recv_message_callback,
                file_rpc=aci.file_rpc,
                aci=aci,
            ),
            "summary": SummaryExecutor(
                name="ACISummaryExec",
                type=ModuleType.EXECUTOR,
                inference_client=inference_client,
            ),
            "fuzz": FuzzGenExecutor(
                name="FuzzGenExecutor",
                type=ModuleType.EXECUTOR,
                file_rpc=aci.file_rpc,
                inference_client=inference_client,
                aci=aci,
            ),
        }

    def prompts(self) -> dict[ACIMode, str]:
        prompts = {
            ACIMode.CONSTRAINED: CONSTRAINED_PROMPT,
            ACIMode.DRIVER: DRIVER_PROMPT,
            ACIMode.EDIT: EDIT_PROMPT,
            ACIMode.NAVIGATE: NAVIGATION_PROMPT,
            ACIMode.CI_BUG_FINDER_QUESTIONS: CI_BUG_FINDER_QUESTIONS_PROMPT,
            ACIMode.CI_BUG_FINDER: CI_BUG_FINDER_PROMPT,
            ACIMode.FULL_SCAN_BUG_FINDER_QUESTIONS: FULL_SCAN_BUG_FINDER_QUESTIONS_PROMPT,
            ACIMode.FULL_SCAN_BUG_FINDER: FULL_SCAN_BUG_FINDER_PROMPT,
            ACIMode.FUZZ_GEN: FUZZ_GEN_PROMPT,
        }

        return prompts

    def enable_humanlayer(self, enabled: bool) -> None:
        self._humanlayer_enabled = enabled

    async def send_aci_status(self, status: str) -> None:
        await self.send_message_callback(
            WSMessage(
                type=WSMessageType.ACI,
                aci=ACIMessage(
                    action=ACIMessage.Action.STATUS,
                    status=status,
                ),
            )
        )

    def set_mode(self, mode: str) -> None:
        self._logger.info(f"SET MODE {mode}")

        if mode == "single":
            self.mode = ACIExecutionMode.SINGLE
        else:
            self.mode = ACIExecutionMode.MULTI

        if self.mode.value == ACIExecutionMode.SINGLE.value:
            self.initial_turns = 3
            self._turns_remaining = 3

    def _turn_wrapper(self, action_func):
        @functools.wraps(action_func)
        async def wrapped(*args, **kwargs):
            action = action_func.__name__

            if self._turns_remaining <= 0:
                self._logger.info(
                    f"Bailing out of action {action} due to no turns remaining."
                )
                raise StopAsyncIteration()

            self._take_turn(action)

            try:
                return await action_func(*args, **kwargs)
            except StopAsyncIteration:
                raise
            except asyncio.CancelledError:
                raise
            except Exception as e:
                self._logger.exception(f"Error in action {action}", exc_info=True)
                return f"Error in action {action}: {e} perhaps your input was incorrect or malformed."

        return wrapped

    async def show_skeleton(self, resp: dict[str, Any]) -> str:
        interesting_files = resp["interesting_files"]

        repo = Repository(
            {
                fn: (await self.file_rpc.read(fn, overlay_modified=True)) or ""
                for fn in interesting_files
            }
        )
        skeleton = await asyncio.to_thread(repo_skeleton, repo)

        return str(skeleton)

    async def toolsets(self) -> dict[ACIMode, list[Any]]:
        sets: dict[ACIMode, list[tuple[Callable, dict]]] = {
            ACIMode.DRIVER: [
                (self.finalize, self.tool_schemas["finalize"]),
                (
                    self.switch_to_editing_mode,
                    self.tool_schemas["switch_to_editing_mode"],
                ),
                (
                    self.switch_to_navigation_mode,
                    self.tool_schemas["switch_to_navigation_mode"],
                ),
                (self.run_command, self.tool_schemas["run_command"]),
            ],
            ACIMode.EDIT: [
                (self.edit_file, self.tool_schemas["edit_file"]),
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.create_file, self.tool_schemas["create_file"]),
                (self.scroll_down_file, self.tool_schemas["scroll_down_file"]),
                (self.scroll_up_file, self.tool_schemas["scroll_up_file"]),
                (
                    self.switch_to_driver_mode,
                    self.tool_schemas["switch_to_driver_mode"],
                ),
                (
                    self.switch_to_navigation_mode,
                    self.tool_schemas["switch_to_navigation_mode"],
                ),
                (self.run_command, self.tool_schemas["run_command"]),
            ],
            ACIMode.NAVIGATE: [
                (self.scroll_down_file, self.tool_schemas["scroll_down_file"]),
                (self.scroll_up_file, self.tool_schemas["scroll_up_file"]),
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.close_file, self.tool_schemas["close_file"]),
                (self.read_file, self.tool_schemas["read_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_line, self.tool_schemas["go_to_line"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (
                    self.switch_to_driver_mode,
                    self.tool_schemas["switch_to_driver_mode"],
                ),
                (
                    self.switch_to_editing_mode,
                    self.tool_schemas["switch_to_editing_mode"],
                ),
                (
                    self.show_skeleton,
                    self.tool_schemas["show_skeleton"],
                ),
                (
                    self.semantic_search,
                    self.tool_schemas["semantic_search"],
                ),
            ],
            ACIMode.CONSTRAINED: [
                (self.edit_file, self.tool_schemas["edit_file"]),
                (self.create_file, self.tool_schemas["create_file"]),
                (self.delete_file, self.tool_schemas["delete_file"]),
                (self.read_file, self.tool_schemas["read_file"]),
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.finalize, self.tool_schemas["finalize"]),
            ],
            ACIMode.CI_BUG_FINDER_QUESTIONS: [
                (self.read_file, self.tool_schemas["read_file"]),
                (self.read_file_diff, self.tool_schemas["read_file_diff"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (self.find_references, self.tool_schemas["find_references"]),
                (
                    self.semantic_search,
                    self.tool_schemas["semantic_search"],
                ),
                (self.add_question, self.tool_schemas["add_question"]),
                (self.finalize, self.tool_schemas["finalize"]),
                (self.think, self.tool_schemas["think"]),
            ],
            ACIMode.CI_BUG_FINDER: [
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.close_file, self.tool_schemas["close_file"]),
                (self.read_file, self.tool_schemas["read_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (self.bug_recurse_fuzz, self.tool_schemas["recurse_fuzz"]),
                (self.search_docs, self.tool_schemas["search_docs"]),
                (self.finalize, self.tool_schemas["finalize"]),
                (self.run_command, self.tool_schemas["run_command"]),
                (self.report_bug_ci, self.tool_schemas["report_bug_ci"]),
            ],
            ACIMode.FULL_SCAN_BUG_FINDER_QUESTIONS: [
                (self.read_file, self.tool_schemas["read_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (self.find_references, self.tool_schemas["find_references"]),
                (
                    self.semantic_search,
                    self.tool_schemas["semantic_search"],
                ),
                (self.add_question, self.tool_schemas["add_question"]),
                (self.finalize, self.tool_schemas["finalize"]),
            ],
            ACIMode.FULL_SCAN_BUG_FINDER: [
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.close_file, self.tool_schemas["close_file"]),
                (self.read_file, self.tool_schemas["read_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_line, self.tool_schemas["go_to_line"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (self.pop_go_to_def, self.tool_schemas["pop_go_to_def"]),
                (self.find_references, self.tool_schemas["find_references"]),
                (self.semantic_search, self.tool_schemas["semantic_search"]),
                (self.resolve_question, self.tool_schemas["resolve_question"]),
                (self.add_memory, self.tool_schemas["add_memory"]),
                (self.bug_recurse_edit, self.tool_schemas["recurse"]),
                (self.bug_recurse_fuzz, self.tool_schemas["recurse_fuzz"]),
                (self.search_docs, self.tool_schemas["search_docs"]),
                (self.finalize, self.tool_schemas["finalize"]),
            ],
            ACIMode.FUZZ_GEN: [
                (self.scroll_down_file, self.tool_schemas["scroll_down_file"]),
                (self.scroll_up_file, self.tool_schemas["scroll_up_file"]),
                (self.switch_file, self.tool_schemas["switch_file"]),
                (self.close_file, self.tool_schemas["close_file"]),
                (self.read_file, self.tool_schemas["read_file"]),
                (self.list_files, self.tool_schemas["list_files"]),
                (self.find_phrase, self.tool_schemas["find_phrase"]),
                (self.go_to_line, self.tool_schemas["go_to_line"]),
                (self.go_to_def, self.tool_schemas["go_to_def"]),
                (self.find_references, self.tool_schemas["find_references"]),
                (
                    self.show_skeleton,
                    self.tool_schemas["show_skeleton"],
                ),
                (
                    self.semantic_search,
                    self.tool_schemas["semantic_search"],
                ),
                (
                    self.run_fuzzer,
                    self.tool_schemas["run_fuzzer"],
                ),
                (
                    self.finalize,
                    self.tool_schemas["finalize"],
                ),
            ],
        }

        if self._humanlayer_enabled:
            sets[ACIMode.DRIVER].extend(
                [
                    (
                        self.reach_out_to_human_for_assistance,
                        self.tool_schemas["reach_out_to_human_for_assistance"],
                    ),
                ]
            )

        for set, tools in sets.items():
            wrapped_tools = list(map(lambda t: (self._turn_wrapper(t[0]), t[1]), tools))

            sets[set] = wrapped_tools

        return sets

    def _lines_in_view(self) -> int:
        if self.mode.value == ACIExecutionMode.SINGLE.value:
            return LINES_IN_VIEW_CONSTRAINED
        else:
            return LINES_IN_VIEW

    async def recurse(self, resp: dict[str, Any]) -> str:
        subtask = resp["subtask"]
        cache = self.cache

        self.recursion_depth += 1
        self._recursive_task = subtask

        if self.recursion_depth > 1:
            self._logger.info("AGENT RECURSION LIMIT REACHED, SKIPPING RECURSE")
            self.recursion_depth -= 1

            return "Recursion depth limit has been reached please try completing the task by yourself."

        self._logger.info(f"GOING RECURSIVE {subtask}")

        executor = self.tool_executors["recurse"]

        # NOP Switch to force recursive subtask into viewer state
        file = await cache.get("active_file")

        await self.manipulate(SwitchAction(file=file), file)

        cur_mode = self._mode
        await self._switch_to_mode(ACIMode.DRIVER, subtask)

        try:
            _response = await executor.process(self.cache, Semaphore(), subtask=subtask)  # type: ignore
        except asyncio.CancelledError:
            raise
        except Exception:
            self._logger.warning("Exception in recursive task.", exc_info=True)

        modified_files = await cache.get("output_modified_files", {})
        pairs = []

        for fn, content in modified_files.items():
            text = f"""
            <file fn={fn}>
            <before>
            {await self.file_rpc.read(fn) or "New file, no previous changes."}
            </before>
            <after>
            {content}
            </after>
            </file>
            """

            pairs.append(text)

        await cache.set("change_log", pairs)

        summary_exec = self.tool_executors["summary"]

        resp = await summary_exec.process(  # type: ignore
            self.cache, Semaphore(), input_message=subtask
        )

        await cache.set("change_log", [])

        self.recursion_depth -= 1
        self._recursive_task = None
        self.finalized = False
        self._attempted_finalize = False
        self._mode = cur_mode

        # Reset viewer_state to have the non recursive task in its most recent state
        file = await cache.get("active_file")

        await self.manipulate(SwitchAction(file=file), file)

        return "<recursion_result>\n" + resp["result"] + "</recursion_result>"

    def set_input_task(self, task: str) -> None:
        self._input_task = task

    async def set_starting_context(
        self, starting_context: dict[str, list[str]]
    ) -> None:
        cache = self.cache
        open_files = []

        starting_context = {
            fn: slices
            for fn, slices in starting_context.items()
            if (await self.file_rpc.read(fn)) is not None
        }

        if starting_context == {}:
            starting_context["placeholder_file"] = [""]

            async with cache.with_suffix(f"file_edit_selection_placeholder_file"):
                await cache.set("lines_above", 0)
                await cache.set("lines_below", 0)
                await cache.set("index", 0)
                await cache.set(
                    "lines",
                    [""],
                )
            open_files.append("placeholder_file")
        else:
            for fn, starting_slices in starting_context.items():
                contents = await self.file_rpc.read(fn)
                assert contents is not None

                lines = contents.split("\n")
                open_files.append(fn)
                ln_range = None

                # Multiple symbols might be in the same file, just start with the earliest for now.
                if starting_slices:
                    ln_range = find_text_chunk(contents, starting_slices[0].split("\n"))
                    if ln_range is None:
                        self._logger.error(f"Could not find starting slice")
                if ln_range is None:
                    ln_range = {
                        "start": 0,
                        "end": min(self._lines_in_view(), len(lines)),
                    }

                slice_in_view = lines[ln_range["start"] : ln_range["end"]]

                if len(slice_in_view) > self._lines_in_view():
                    slice_in_view = slice_in_view[: self._lines_in_view()]
                    start = ln_range["start"]
                    end = ln_range["start"] + self._lines_in_view()
                else:
                    view_remaining = self._lines_in_view() - len(slice_in_view)

                    before = view_remaining // 2
                    after = math.ceil(view_remaining / 2)

                    if ln_range["start"] - before <= 0:
                        start = 0
                        end = self._lines_in_view()
                    else:
                        start = max(0, ln_range["start"] - before)
                        end = min(len(lines), ln_range["end"] + after)

                index = end
                lines_above = start
                lines_below = max(0, len(lines) - index)

                async with cache.with_suffix(f"file_edit_selection_{fn}"):
                    await cache.set("lines_above", lines_above)
                    await cache.set("lines_below", lines_below)
                    await cache.set("index", index)
                    await cache.set("lines", lines)

        await cache.set("code_analysis", [])
        await cache.set("test_output", "")
        await cache.set("system_analysis_output", "")
        await cache.set("viewer_open_files", open_files)
        starting_file = list(starting_context.keys())[0]

        await cache.set("active_file", starting_file)
        await cache.set("looked_at_files", [starting_file])

        await self.manipulate(StartAction(), starting_file)

    @classmethod
    async def create(
        cls,
        cache: Cache,
        inference_client: InferenceClient,
        inference_client_factory: Callable[[str], InferenceClient],
        file_rpc: FileRPC,
        send_message_callback: Callable[
            [WSMessage], Awaitable[None]
        ] = null_send_callback,
        recv_message_callback: Callable[[], Awaitable[WSMessage]] = null_recv_callback,
        interactive_mode=False,
        run_tests_on_finalize=False,
        initial_mode=ACIMode.DRIVER,
        initial_turns=40,
    ) -> "ACI":
        if send_message_callback is None:

            async def _send_message_callback(message):
                pass

            send_message_callback = _send_message_callback

        await cache.set("viewer_history", [])

        instance = cls(
            cache=cache,
            viewer_state="",
            tool_executors={},
            send_message_callback=send_message_callback,
            recv_message_callback=recv_message_callback,
            interactive_mode=interactive_mode,
            run_tests_on_finalize=run_tests_on_finalize,
            file_rpc=file_rpc,
            initial_turns=initial_turns,
        )
        instance._logger = logging.getLogger(__name__).getChild(
            await cache.get("request_id")
        )

        instance._mode = initial_mode
        instance._step_count = 0
        instance._turns_remaining = initial_turns

        executors = await ACI._create_executors(
            inference_client,
            inference_client_factory,
            send_message_callback,
            recv_message_callback,
            instance,
        )

        instance.tool_executors = executors

        return instance

    def _take_turn(self, action: str) -> None:
        if (
            action
            in (
                "create_file",
                "edit_file",
                "delete_file",
                "run_command",
                "recurse",
                "reach_out_to_human_for_assistance",
                "report_bug_ci",
                "resolve_question",
            )
            and self.mode.value == ACIExecutionMode.MULTI.value
        ):
            self._turns_remaining -= 1

        if (
            action
            in (
                "create_file",
                "edit_file",
                "delete_file",
            )
            and self.mode.value == ACIExecutionMode.SINGLE.value
        ):
            self._turns_remaining -= 1

        self._logger.debug(f"TURNS REMAINING {self._turns_remaining}")

    def validate_llm_call(
        self, resp, schema
    ) -> tuple[Literal[True], None] | tuple[Literal[False], str]:
        for key in schema["properties"].keys():
            if key not in resp and key in schema["required"]:
                self._logger.info(f"Missing key {key} in response.")
                return False, key

        return True, None

    def replace_closest_edit_distance(
        self, whole: str, part: str, replace: str
    ) -> Optional[str]:
        similarity_thresh = 0.8
        whole_lines = whole.split("\n")
        part_lines = part.split("\n")

        replace_lines = replace.split("\n")

        max_similarity = 0.0
        most_similar_chunk_start = -1
        most_similar_chunk_end = -1

        scale = 0.1
        min_len = math.floor(len(part_lines) * (1 - scale))
        max_len = math.ceil(len(part_lines) * (1 + scale))

        for length in range(min_len, max_len):
            for i in range(len(whole_lines) - length + 1):
                chunk = "".join(whole_lines[i : i + length])
                part_to_match = "".join(part_lines)

                similarity = SequenceMatcher(None, chunk, part_to_match).ratio()

                if similarity > max_similarity and similarity:
                    max_similarity = similarity
                    most_similar_chunk_start = i
                    most_similar_chunk_end = i + length

        if max_similarity < similarity_thresh:
            return None

        modified_whole = (
            whole_lines[:most_similar_chunk_start]
            + replace_lines
            + whole_lines[most_similar_chunk_end:]
        )

        return "\n".join(modified_whole)

    def set_pinned_files(self, pinned_files: dict[str, str]) -> None:
        self._pinned_files = pinned_files

    async def manipulate(self, action: ACIAction, fn: str) -> str:
        cache = self.cache
        open_files = await cache.get("viewer_open_files", [])

        if self.recursion_depth == 0:
            input_task = await cache.get("input_message")
        else:
            input_task = self._recursive_task

        output_modified_files = await cache.get("output_modified_files", {})
        analysis_lines = await cache.get("code_analysis", [])
        test_output = await cache.get("test_output")
        system_analysis = ""

        self._step_count += 1

        files_with_id = []
        for idx, file in enumerate(open_files):
            files_with_id.append(f"{idx}: {file}")

        if not isinstance(action, (CreateAction, OpenAction)):
            async with cache.with_suffix(f"file_edit_selection_{fn}"):
                index = await cache.get("index")
                lines_below = await cache.get("lines_below")
                lines_above = await cache.get("lines_above")
                lines = await cache.get("lines")

        match action:
            case StartAction():
                self._logger.debug(f"START {fn}")
                new_lines = lines[lines_above:index]

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.START,
                            status="",
                            files=open_files,
                            active_file=fn,
                            new_contents="\n".join(lines),
                            scroll_position=lines_above,
                        ),
                    )
                )
            case SwitchAction():
                self._logger.debug(f"SWITCH {fn}")
                await cache.set("active_file", action.file)
                await cache.set(
                    "looked_at_files",
                    await cache.get("looked_at_files") + [action.file],
                )
                index = min(index, len(lines))
                # TODO: adjust below/above
                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.SWITCH,
                            status=f"Looking through {action.file}",
                            active_file=action.file,
                            new_contents="\n".join(lines),
                            scroll_position=lines_above,
                        ),
                    )
                )
                new_lines = lines[:index][-self._lines_in_view() :]
            case TestAction():
                self._logger.debug(f"TEST {fn}")
                test_output = action.test_output
                await cache.set("test_output", test_output)
                new_lines = lines[:index][-self._lines_in_view() :]

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.TEST,
                            status=f"Ran tests for {fn}",
                            test_output=test_output,
                        ),
                    )
                )
            case SystemAnalysisAction():
                self._logger.debug(f"SYSTEM_ANALYSIS")
                new_lines = lines[:index][-self._lines_in_view() :]

                system_analysis = action.system_analysis_output

                await cache.set("system_analysis_output", system_analysis)
            case CreateAction() | OpenAction():
                open_files = await cache.get("viewer_open_files", [])
                if action.file not in open_files:
                    open_files.append(action.file)
                    files_with_id.append(f"{len(open_files) - 1}: {action.file}")

                await cache.set("viewer_open_files", open_files)

                lines = action.content.split("\n")

                index = min(len(lines), self._lines_in_view())
                lines_above = 0
                lines_below = len(lines) - index

                async with cache.with_suffix(f"file_edit_selection_{action.file}"):
                    await cache.set("lines_above", lines_above)
                    await cache.set("lines_below", lines_below)
                    await cache.set("index", index)
                    await cache.set("lines", lines)

                status = f"Opened {action.file}"

                if isinstance(action, CreateAction):
                    output_modified_files[action.file] = action.content
                    status = f"Created {action.file}"
                    await cache.set("output_modified_files", output_modified_files)

                await cache.set("active_file", action.file)
                await cache.set(
                    "looked_at_files",
                    await cache.get("looked_at_files") + [action.file],
                )

                new_lines = lines[lines_above:index]

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.CREATE,
                            status=status,
                            files=open_files,
                            active_file=action.file,
                            scroll_position=index,
                            new_contents=action.content,
                        ),
                    )
                )

            case EditAction():
                self._logger.debug(f"EDIT: {fn}")

                start_chunk = lines[:lines_above]
                end_chunk = lines[index:]

                new_content = []
                new_content.extend(start_chunk)
                viewer_lines = lines[lines_above:][: self._lines_in_view()]

                file_content = await self.file_rpc.read(fn, overlay_modified=True)
                if file_content is None:
                    self._turns_remaining += 1
                    self._logger.warning(f"Failed to read {fn} in EDIT.")
                    return "It appears the file is empty somehow this is an invalid state, please try to cope with this as best you can but otherwise. Cede control back to the driver."

                try:
                    source_file = SourceFile(fn, file_content.encode("utf-8"))
                    pattern = source_file.analyze_whitespace_pattern()
                except UnknownExtensionException:
                    pattern = SourceFile.WhitespacePattern()

                text = pattern.line_ending.join(viewer_lines)
                normalized_search = action.lines_to_replace
                normalized_replace = action.replace_text

                text = self.replace_closest_edit_distance(
                    text, normalized_search, normalized_replace
                )

                if not text:
                    self._turns_remaining += 1
                    return "Tried fuzzy replace and was not able to find a match, please double check the lines you are trying to replace."

                # Use normalized versions for replacement with consistent whitespace patterns
                try:
                    new_lines = text.split(pattern.line_ending)
                    new_content.extend(new_lines)
                    new_content.extend(end_chunk)

                    output_modified_files[fn] = pattern.line_ending.join(new_content)
                except Exception:
                    self._logger.exception(
                        f"Final whitespace normalization failed, falling back to basic normalization"
                    )
                    # Fallback to basic normalization
                    text = text.replace(normalized_search, action.replace_text.rstrip())
                    new_lines = text.split("\n")
                    new_content.extend(new_lines)
                    new_content.extend(end_chunk)
                    output_modified_files[fn] = "\n".join(new_content)

                nl_len = len(new_lines)

                if nl_len < self._lines_in_view():
                    diff = self._lines_in_view() - nl_len
                    line_additions = lines[index:][:diff]
                    new_lines.extend(line_additions)
                    lines_below -= diff
                    lines_below = max(lines_below, 0)
                elif nl_len > self._lines_in_view():
                    diff = nl_len - self._lines_in_view()
                    new_lines = new_lines[: self._lines_in_view()]
                    lines_below += diff

                lines = new_content

                marker = max(len(new_lines), self._lines_in_view())
                index = lines_above + marker

                analysis_lines = []

                if not os.environ.get("DISABLE_LSP"):
                    self._logger.debug("In code analysis stuff.")
                    feature_id = await cache.get("feature_id")
                    feature = FeatureEntity.get(feature_id)
                    assert feature is not None
                    git_url = await cache.get("repo_url")

                    try:
                        analysis_lines = [
                            repr(d)
                            for d in (
                                await self._analysis_client.lsp_analyze(
                                    git_url,
                                    feature_id,
                                    feature.name,
                                    fn,
                                    output_modified_files,
                                )
                            ).diagnostics
                        ]
                        self._logger.debug(f"analyze lines: {analysis_lines}")
                    except Exception:
                        self._logger.warning("Exception in analysis", exc_info=True)
                        pass

                await cache.set("output_modified_files", output_modified_files)
                async with cache.with_suffix(f"file_edit_selection_{fn}"):
                    await cache.set("lines_below", lines_below)
                    await cache.set("lines", new_content)
                    await cache.set("index", index)

                edit_line = "\n".join(new_content)[
                    : "\n".join(new_content).find(action.replace_text)
                ].count("\n")
                edit_len = action.replace_text.count("\n")
                if action.replace_text.startswith(action.lines_to_replace):
                    edit_line += action.lines_to_replace.count("\n")
                    edit_len -= action.lines_to_replace.count("\n")
                elif action.replace_text.endswith(action.lines_to_replace):
                    edit_len -= action.lines_to_replace.count("\n")

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.EDIT,
                            status=f"Made changes to {fn}",
                            new_contents="\n".join(new_content),
                            scroll_position=edit_line,
                            changed_range=(
                                edit_line,
                                edit_line + edit_len,
                            ),
                        ),
                    )
                )

                await cache.set("code_analysis", analysis_lines)
                await cache.set("test_output", "")

                test_output = ""

            case ScrollDownAction() | ScrollUpAction():
                if isinstance(action, ScrollDownAction):
                    scroll = action.scroll
                else:
                    scroll = -action.scroll
                new_index = max(self._lines_in_view(), index + scroll)

                # Ensure the new_index is an index inside of the actual lines.
                new_index = min(new_index, len(lines) - 1)

                new_lines = lines[:new_index][-self._lines_in_view() :]

                lines_above = max(0, new_index - self._lines_in_view())
                lines_below = max(0, len(lines) - new_index)

                self._logger.debug(f"SCROLL: {fn}")

                async with cache.with_suffix(f"file_edit_selection_{fn}"):
                    await cache.set("lines_above", lines_above)
                    await cache.set("lines_below", lines_below)
                    await cache.set("index", new_index)

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.SCROLL,
                            status=f"Looking through {fn}",
                            scroll_position=lines_above,
                        ),
                    )
                )
            case JumpAction():
                self._logger.debug(f"JUMP: {fn}")
                self._logger.debug(f"LINE: {action.line}")

                # Center the view on the target line
                view_start = max(0, action.line - (self._lines_in_view() // 2))
                new_index = min(view_start + self._lines_in_view(), len(lines))
                new_lines = lines[view_start:new_index]

                lines_above = view_start
                lines_below = max(0, len(lines) - new_index)

                async with cache.with_suffix(f"file_edit_selection_{fn}"):
                    await cache.set("lines_above", lines_above)
                    await cache.set("lines_below", lines_below)
                    await cache.set("index", new_index)
                await cache.set("active_file", fn)
                await cache.set(
                    "looked_at_files", await cache.get("looked_at_files") + [fn]
                )

                await self.send_message_callback(
                    WSMessage(
                        type=WSMessageType.ACI,
                        aci=ACIMessage(
                            action=ACIMessage.Action.SWITCH,
                            status=f"Looking through {fn}",
                            active_file=fn,
                            scroll_position=view_start,
                            new_contents="\n".join(lines),
                        ),
                    )
                )

        thoughts = await self.cache.get("driver_subsystem_communications", "")

        toolset = (await self.toolsets())[self._mode]

        if USE_GASP_TOOLS:
            # When using gasp tools, toolset is a tuple (parser_func, reducer_func)
            # Extract tool names from the mode's union type
            from daneel.executors.aci.aci_tool_models import (
                MODE_TOOL_UNIONS,
                MODE_TOOL_UNIONS_WITH_HUMAN,
            )

            if self._humanlayer_enabled and self._mode == ACIMode.DRIVER:
                tool_union = MODE_TOOL_UNIONS_WITH_HUMAN[self._mode]
            else:
                tool_union = MODE_TOOL_UNIONS[self._mode]
            # Get the __args__ from the Union to get individual tool types
            tool_types = tool_union.__value__.__args__
            tool_names = "|".join([t.__name__ for t in tool_types]) + "\n"
        else:
            # Original format
            tool_names = "|".join([tool[1]["name"] for tool in toolset]) + "\n"

        if self._mode in (
            ACIMode.FULL_SCAN_BUG_FINDER,
            ACIMode.FULL_SCAN_BUG_FINDER_QUESTIONS,
            ACIMode.CI_BUG_FINDER_QUESTIONS,
            ACIMode.CI_BUG_FINDER,
        ):
            viewer_state = BugFindingVisualizer.generate_viewer_state(
                files_with_id=files_with_id,
                fn=fn,
                new_lines=new_lines,
                current_questions=await self.cache.get("current_questions"),
                memories=await self.cache.get("bug_finding_memories", []),
                call_stack=await self.cache.get("go_to_def_stack", []),
                available_tools=tool_names,
                system_analysis=system_analysis,
            )
        elif self._mode == ACIMode.FUZZ_GEN:
            viewer_state = FuzzVisualizer.generate_viewer_state(
                input_task=input_task,
                files_with_id=files_with_id,
                fn=fn,
                lines_above=lines_above,
                lines_below=lines_below,
                new_lines=new_lines,
                test_output=test_output,
                system_analysis=system_analysis,
                available_tools=tool_names,
            )
        else:
            viewer_state = ACIVisualizer.generate_viewer_state(
                input_task=input_task,
                files_with_id=files_with_id,
                fn=fn,
                lines_above=lines_above,
                lines_below=lines_below,
                new_lines=new_lines,
                analysis_lines=analysis_lines,
                test_output=test_output,
                system_analysis=system_analysis,
                available_tools=tool_names,
                thoughts=thoughts,
                mode=self._mode,
                turns_remaining=self._turns_remaining,
            )

        self.viewer_state = viewer_state
        await trace_output(viewer_state, "aci_output")
        viewer_history = await self.cache.get("viewer_history", [])
        viewer_history.append(viewer_state)
        await self.cache.set("viewer_history", viewer_history)

        self._last_shown_viewer_state = self._step_count

        return viewer_state

    def normalize_filename(self, filename: str) -> str:
        # strip potential leading ./
        if not is_url(filename):
            filename = filename.lstrip("/")
            if filename.startswith("./"):
                filename = filename[2:]
        return filename

    async def edit_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache
        file = self.normalize_filename(resp["file"])

        if is_url(file):
            self._turns_remaining += 1  # Don't penalize for trying to edit a URL
            return "Cannot edit a URL. URLs are read-only."

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["edit_file"]["input_schema"]
        )

        if not passed:
            self._turns_remaining += 1
            return f"{failing_key} is required for 'edit_file', please try again with the correct parameters."

        id = resp["id"]
        step = resp["step"]
        lines_to_replace = resp["lines_to_replace"]
        replace_text = resp["replace_text"]

        active_file = await cache.get("active_file")
        if is_url(active_file):
            self._turns_remaining += 1
            return (
                "Cannot edit a URL. The active resource is a URL, which is read-only."
            )

        if lines_to_replace.strip() == "BISMUTH_DELETED_FILE":
            self._turns_remaining += 1
            return "That file has been previously deleted you either need to recreate it or create an entirely new file."

        if not lines_to_replace.strip():
            self._turns_remaining += 1
            return "You must provide content in the lines to replace."

        lines_to_replace = lines_to_replace.rstrip()
        replace_text = replace_text.rstrip()
        if active_file != file:
            output_modified_files = await cache.get("output_modified_files", {})
            open_files = await cache.get("viewer_open_files", [])
            if (
                file not in output_modified_files
                or output_modified_files[file] == "BISMUTH_DELETED_FILE"
            ):
                self._turns_remaining += 1
                return "That file was deleted or closed, please switch to a different file."

            await self.switch_file({"file_id": open_files.index(file)})

        try:
            action = EditAction(
                lines_to_replace=lines_to_replace,
                replace_text=replace_text,
                file=file,
            )
            content = await self.manipulate(action, active_file)
        except ValueError as e:
            self._logger.exception(f"Error in manipulate(EditAction)")
            return str(e)

        session_id = await cache.get("msg_session_id")
        chat_session = ChatSessionEntity.get(session_id)
        assert chat_session is not None
        session_context = chat_session.get_context()

        context_edited_files = session_context.get("edited_files", [])
        context_edited_files.append(
            {
                "file": active_file,
                "step": step,
            }
        )

        session_context["edited_files"] = context_edited_files
        chat_session.set_context(session_context)

        locators = await cache.get("locators")
        locators.append(
            {
                "id": id,
                "file": active_file,
                "step": step,
                "lines_to_replace": lines_to_replace,
                "replace": replace_text,
            }
        )
        await self.update_change_log(step)

        await cache.set("locators", locators)

        return content

    async def switch_file(self, resp: dict[str, Any]) -> str:
        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["switch_file"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'switch_file', please try again with the correct parameters."

        file_id = int(resp["file_id"])
        cache = self.cache
        open_files = await cache.get("viewer_open_files", [])

        if file_id >= len(open_files) or file_id < 0:
            files_with_id = []
            for idx, file in enumerate(open_files):
                files_with_id.append(f"{idx}: {file}")
            return "Invalid file id. Valid files are:\n" + "\n".join(files_with_id)

        file = open_files[file_id]

        if file == "CLOSED":
            return "That file has been closed please try switching to a different file."

        old_active_file = await cache.get("active_file", "")

        if file == old_active_file:
            return "You are already on that file. Please take another action."

        content = await self.manipulate(SwitchAction(file=file), file)

        return content

    async def read_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["read_file"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'read_file', please try again with the correct parameters."

        file_path = self.normalize_filename(resp["file"])
        open_files = await cache.get("viewer_open_files", [])
        file_content = None
        parsed_url = urlparse(file_path)
        hostname = parsed_url.hostname

        # Check internal ACI instance cache first
        if is_url(file_path) and file_path in self._url_content_cache:
            file_content = self._url_content_cache[file_path]
        elif is_url(file_path):
            self._logger.info(f"Opening URL: {file_path}")
            project = await self._get_project()

            # Try platform-specific handlers first
            if hostname == "github.com":
                file_content = await self._fetch_github_url_content(
                    file_path, parsed_url
                )
            elif hostname and hostname.endswith(".atlassian.net"):
                file_content = await self._fetch_jira_url_content(file_path, parsed_url)
            elif hostname == "linear.app":
                file_content = await self._fetch_linear_url_content(
                    file_path, parsed_url
                )
            else:  # Check for GitLab based on integration server url
                if project and project.gitlab_app_install_id:
                    gitlab_install = GitLabAppInstallEntity.get(
                        project.gitlab_app_install_id
                    )
                    if gitlab_install and gitlab_install.server_url:
                        integrated_gitlab_hostname = urlparse(
                            gitlab_install.server_url
                        ).hostname
                        if hostname == integrated_gitlab_hostname:
                            self._logger.info(
                                f"URL matches configured GitLab instance: {gitlab_install.server_url}"
                            )
                            file_content = await self._fetch_gitlab_url_content(
                                file_path, parsed_url
                            )

            if (
                file_content is None
            ):  # Fallback to generic fetching if platform-specific failed or no handler
                await self.send_aci_status(f"Fetching content from {file_path}...")
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            file_path,
                            timeout=aiohttp.ClientTimeout(total=10),
                            headers={
                                "User-Agent": "Mozilla/5.0 (compatible; Bismuth/1.0; +https://bismuth.sh)"
                            },
                        ) as response:
                            if response.status != 200:
                                self._logger.info(
                                    f"Failed to fetch URL {file_path}: {response.status}"
                                )
                                return (
                                    f"Error fetching URL {file_path}: {response.status}"
                                )
                            try:
                                html_content = await response.text()
                            except UnicodeDecodeError:
                                return f"{file_path} is not text."

                            soup = BeautifulSoup(html_content, "html.parser")
                            for script_or_style in soup(["script", "style"]):
                                script_or_style.decompose()

                            placeholders = {}
                            for a_tag in soup.find_all("a"):
                                href = a_tag.get("href")  # type: ignore
                                # Get clean text for the link, stripping any inner tags like <b> etc.
                                link_text = a_tag.get_text(strip=True)
                                if (
                                    href
                                ):  # Only process <a> tags that have an href attribute
                                    placeholder = f"__BISMUTH_LINK_PLACEHOLDER_{uuid.uuid4().hex}__"
                                    placeholders[placeholder] = (
                                        f'<a href="{href}">{link_text}</a>'
                                    )
                                    # Replace the <a> tag with a NavigableString containing the placeholder.
                                    # This ensures soup.get_text() will include the placeholder.
                                    a_tag.replace_with(NavigableString(placeholder))
                                # If <a> tag has no href, its text content will be extracted by soup.get_text() naturally.

                            # Get text from the soup, now containing placeholders instead of <a> tags.
                            text_with_placeholders = soup.get_text(
                                separator="\n", strip=True
                            )

                            # Substitute placeholders back with their formatted link strings.
                            final_text = text_with_placeholders
                            for placeholder, link_string in placeholders.items():
                                final_text = final_text.replace(
                                    placeholder, link_string
                                )

                            file_content = final_text
                            if len(file_content) > 1000000:
                                return "URL content too large."
                            self._url_content_cache[file_path] = file_content
                            ExternalContextEntity(
                                url=file_path,
                                content=file_content,
                                organization_id=cast(int, project.organization_id),
                                chat_session_id=await cache.get("msg_session_id"),
                            ).persist()
                except aiohttp.ClientError as e:
                    self._logger.error(f"Error fetching URL {file_path}: {e}")
                    return f"Error fetching URL {file_path}: {e}"
                except Exception as e:
                    self._logger.error(f"Error processing URL {file_path}: {e}")
                    return f"Error processing URL {file_path}: {e}"

        else:  # It's a regular file
            file_content = await self.file_rpc.read(file_path, overlay_modified=True)

        if not file_content or file_content == "BISMUTH_DELETED_FILE":
            return "File doesn't exist - perhaps you have the wrong path?"

        if file_path in open_files:
            content = await self.manipulate(SwitchAction(file=file_path), file_path)
        else:
            content = await self.manipulate(
                OpenAction(content=file_content, file=file_path), file_path
            )

            session_id = await cache.get("msg_session_id")
            chat_session = ChatSessionEntity.get(session_id)
            assert chat_session is not None
            session_context = chat_session.get_context()
            context_opened_files = session_context.get("opened_files", [])
            context_opened_files.append(file_path)

            session_context["opened_files"] = context_opened_files
            chat_session.set_context(session_context)

        return content

    async def _get_project(self) -> ProjectEntity:
        feature_id = await self.cache.get("feature_id")
        feature = FeatureEntity.get(feature_id)
        assert feature is not None
        return feature.project

    async def _fetch_github_url_content(
        self, url: str, parsed_url: Any
    ) -> Optional[str]:
        self._logger.info(f"Attempting to fetch GitHub content for: {url}")
        project = await self._get_project()
        if not project or not project.github_app_install_id or not project.github_repo:
            self._logger.warning("Missing GitHub app install or repo for project.")
            return None

        path_parts = parsed_url.path.strip("/").split("/")
        if len(path_parts) < 4:
            self._logger.warning(f"GitHub URL path too short: {parsed_url.path}")
            return None  # Expected owner/repo/type/number

        owner = path_parts[0]
        repo_name = path_parts[1]
        item_type = path_parts[2]
        item_number_str = path_parts[3]
        try:
            item_number = int(item_number_str)
        except ValueError:
            self._logger.warning(
                f"Invalid item number in GitHub URL: {item_number_str}"
            )
            return None

        try:
            gh_install = GitHubAppInstallEntity.get(project.github_app_install_id)
            if not gh_install:
                self._logger.warning(
                    f"GitHubAppInstallEntity not found for ID: {project.github_app_install_id}"
                )
                return None

            gh = github_service.get_install_github(gh_install.installation_id)
            repo = await asyncio.to_thread(
                gh.get_repo, f"{owner}/{repo_name}"
            )  # Use configured repo

            content_parts = []
            if item_type == "issues":
                issue = await asyncio.to_thread(repo.get_issue, item_number)
                expanded_body = github_service.expand_github_references(
                    issue.body, project
                )
                content_parts.append(f"Title: {issue.title}\n\n{expanded_body}")
                comments = list(await asyncio.to_thread(issue.get_comments))
                if comments:
                    content_parts.append("\n\n--- COMMENTS ---")
                    for comment in comments:
                        expanded_comment_body = github_service.expand_github_references(
                            comment.body, project
                        )
                        content_parts.append(
                            f"\n\nUser: {comment.user.login}\nDate: {comment.created_at}\n{expanded_comment_body}"
                        )
            elif item_type == "pull":
                pr = await asyncio.to_thread(repo.get_pull, item_number)
                expanded_body = github_service.expand_github_references(
                    pr.body, project
                )
                content_parts.append(f"Title: {pr.title}\n\n{expanded_body}")
                issue_comments = list(await asyncio.to_thread(pr.get_issue_comments))
                if issue_comments:
                    content_parts.append("\n\n--- GENERAL PR COMMENTS ---")
                    for comment in issue_comments:
                        expanded_comment_body = github_service.expand_github_references(
                            comment.body, project
                        )
                        content_parts.append(
                            f"\n\nUser: {comment.user.login}\nDate: {comment.created_at}\n{expanded_comment_body}"
                        )
            else:
                self._logger.warning(f"Unsupported GitHub item type: {item_type}")
                return None
            return "\n".join(content_parts)
        except Exception as e:
            self._logger.error(
                f"Error fetching GitHub content for {url}: {e}", exc_info=True
            )
            return f"Error fetching GitHub content: {e}"

    async def _fetch_gitlab_url_content(
        self, url: str, parsed_url: Any
    ) -> Optional[str]:
        self._logger.info(f"Attempting to fetch GitLab content for: {url}")
        project = await self._get_project()
        if not project or not project.gitlab_app_install_id or not project.gitlab_repo:
            self._logger.warning("Missing GitLab app install or repo for project.")
            return None

        path_parts = parsed_url.path.strip("/").split("/")
        # Example: /owner/repo/-/issues/1 or /owner/repo/-/merge_requests/2
        # Find '-' and then the type and number
        try:
            separator_index = path_parts.index("-")
            item_type_str = path_parts[separator_index + 1]  # issues or merge_requests
            item_number_str = path_parts[separator_index + 2]
            item_number = int(item_number_str)
        except (ValueError, IndexError):
            self._logger.warning(f"Could not parse GitLab URL path: {parsed_url.path}")
            return None

        try:
            gl_install = GitLabAppInstallEntity.get(project.gitlab_app_install_id)
            if not gl_install or not gl_install.pat or not gl_install.server_url:
                self._logger.warning(
                    f"GitLabAppInstallEntity not found or incomplete for ID: {project.gitlab_app_install_id}"
                )
                return None

            gl = gitlab.Gitlab(gl_install.server_url, private_token=gl_install.pat)
            await asyncio.to_thread(gl.auth)  # Check auth

            # Use project.gitlab_repo which should be path_with_namespace
            gl_project = await asyncio.to_thread(gl.projects.get, project.gitlab_repo)

            content_parts = []
            if item_type_str == "issues":
                issue = await asyncio.to_thread(gl_project.issues.get, item_number)
                expanded_description = expand_gitlab_references(
                    issue.description, project
                )
                content_parts.append(f"Title: {issue.title}\n\n{expanded_description}")
                notes = list(await asyncio.to_thread(issue.notes.list, all=True))
                if notes:
                    content_parts.append("\n\n--- COMMENTS ---")
                    for note in notes:
                        expanded_note_body = expand_gitlab_references(
                            note.body, project
                        )
                        content_parts.append(
                            f"\n\nUser: {note.author['username']}\nDate: {note.created_at}\n{expanded_note_body}"
                        )
            elif item_type_str == "merge_requests":
                mr = await asyncio.to_thread(gl_project.mergerequests.get, item_number)
                expanded_description = expand_gitlab_references(mr.description, project)
                content_parts.append(f"Title: {mr.title}\n\n{expanded_description}")
                notes = list(await asyncio.to_thread(mr.notes.list, all=True))
                if notes:
                    content_parts.append("\n\n--- COMMENTS ---")
                    for note in notes:
                        expanded_note_body = expand_gitlab_references(
                            note.body, project
                        )
                        content_parts.append(
                            f"\n\nUser: {note.author['username']}\nDate: {note.created_at}\n{expanded_note_body}"
                        )
            else:
                self._logger.warning(f"Unsupported GitLab item type: {item_type_str}")
                return None
            return "\n".join(content_parts)
        except Exception as e:
            self._logger.error(
                f"Error fetching GitLab content for {url}: {e}", exc_info=True
            )
            return f"Error fetching GitLab content: {e}"

    async def _fetch_linear_url_content(
        self, url: str, parsed_url: Any
    ) -> Optional[str]:
        self._logger.info(f"Attempting to fetch Linear content for: {url}")
        project = await self._get_project()
        if (
            not project
            or not project.organization
            or not project.organization.linear_install_id
        ):
            self._logger.warning("Missing Linear app install for organization.")
            return None

        # URL like: /<team-name>/issue/ISSUE_ID_OR_SLUG/...
        path_parts = parsed_url.path.strip("/").split("/")
        if len(path_parts) < 3 or path_parts[1] != "issue":
            self._logger.warning(
                f"Could not parse Linear URL path for issue ID: {parsed_url.path}"
            )
            return None
        issue_identifier = path_parts[2]  # This could be slug (TEAM-123) or UUID

        try:
            linear_install = LinearAppInstallEntity.get(
                project.organization.linear_install_id
            )
            if not linear_install or not linear_install.token:
                self._logger.warning(
                    f"LinearAppInstallEntity not found or token missing for ID: {project.organization.linear_install_id}"
                )
                return None

            transport = GQLAIOHTTPTransport(
                url="https://api.linear.app/graphql",
                headers={"Authorization": f"Bearer {linear_install.token}"},
            )
            async with GQLClient(
                transport=transport, fetch_schema_from_transport=False
            ) as gql_session:
                # Try to get issue by ID (UUID or team-number identifier)
                # Linear API is flexible with identifier for issues
                query = gql(
                    """
                    query GetIssue($identifier: String!) {
                        issue(id: $identifier) {
                            title
                            description
                            comments {
                                nodes {
                                    body
                                    user { name email }
                                    createdAt
                                }
                            }
                        }
                    }
                """
                )
                variables = {"identifier": issue_identifier}
                result = await gql_session.execute(query, variable_values=variables)

                issue_data = result.get("issue")
                if not issue_data:
                    self._logger.warning(
                        f"Linear issue not found for identifier: {issue_identifier}"
                    )
                    return f"Linear issue not found: {issue_identifier}"

                content_parts = [f"Title: {issue_data['title']}"]
                if issue_data.get("description"):
                    content_parts.append(f"\n{issue_data['description']}")

                comments = issue_data.get("comments", {}).get("nodes", [])
                if comments:
                    content_parts.append("\n\n--- COMMENTS ---")
                    for comment in comments:
                        user_name = comment.get("user", {}).get("name", "Unknown User")
                        content_parts.append(
                            f"\n\nUser: {user_name}\nDate: {comment['createdAt']}\n{comment['body']}"
                        )
                return "\n".join(content_parts)

        except Exception as e:
            self._logger.error(
                f"Error fetching Linear content for {url}: {e}", exc_info=True
            )
            return f"Error fetching Linear content: {e}"

    async def _fetch_jira_url_content(self, url: str, parsed_url: Any) -> Optional[str]:
        self._logger.info(f"Attempting to fetch Jira content for: {url}")
        project = await self._get_project()
        if not project or not project.atlassian_install_id:
            self._logger.warning("Missing Atlassian (Jira) app install for project.")
            return None

        # URL usually /browse/ISSUE-KEY
        path_parts = parsed_url.path.strip("/").split("/")
        if len(path_parts) < 2 or path_parts[0].lower() != "browse":
            self._logger.warning(
                f"Could not parse Jira URL path for issue key: {parsed_url.path}"
            )
            return None
        issue_key = path_parts[1]

        try:
            atlassian_install = AtlassianAppInstallEntity.get(
                project.atlassian_install_id
            )
            if (
                not atlassian_install
                or not atlassian_install.api_base
                or not atlassian_install.jira_token
            ):
                self._logger.warning(
                    f"AtlassianAppInstallEntity not found or incomplete for ID: {project.atlassian_install_id}"
                )
                return None

            headers = {
                "Authorization": f"Bearer {atlassian_install.jira_token}",
                "Accept": "application/json",
            }
            async with httpx.AsyncClient() as client:
                # Fetch issue details
                issue_url = f"{atlassian_install.api_base}/rest/api/3/issue/{issue_key}"
                self._logger.debug(f"Fetching Jira issue: {issue_url}")
                issue_response = await client.get(issue_url, headers=headers)
                if issue_response.status_code != 200:
                    self._logger.error(
                        f"Jira API error fetching issue {issue_key}: {issue_response.status_code} - {issue_response.text}"
                    )
                    return f"Jira API error fetching issue {issue_key}: {issue_response.status_code}"

                issue_data = issue_response.json()
                fields = issue_data.get("fields", {})
                summary = fields.get("summary", "No summary")
                description_adf = fields.get("description")
                description_md = (
                    adf_to_md(description_adf) if description_adf else "No description."
                )
                expanded_description_md = expand_jira_references(
                    description_md, project
                )

                content_parts = [f"Title: {summary}\n\n{expanded_description_md}"]

                # Fetch comments
                comments_url = (
                    f"{atlassian_install.api_base}/rest/api/3/issue/{issue_key}/comment"
                )
                self._logger.debug(f"Fetching Jira comments: {comments_url}")
                comments_response = await client.get(comments_url, headers=headers)
                if comments_response.status_code == 200:
                    comments_data = comments_response.json()
                    comments = comments_data.get("comments", [])
                    if comments:
                        content_parts.append("\n\n--- COMMENTS ---")
                        for comment in comments:
                            author_name = comment.get("author", {}).get(
                                "displayName", "Unknown User"
                            )
                            created_date = comment.get("created", "N/A")
                            body_adf = comment.get(
                                "body"
                            )  # Jira comments are also in ADF
                            body_md = (
                                adf_to_md(body_adf) if body_adf else "Empty comment"
                            )
                            expanded_body_md = expand_jira_references(body_md, project)
                            content_parts.append(
                                f"\n\nUser: {author_name}\nDate: {created_date}\n{expanded_body_md}"
                            )
                else:
                    self._logger.warning(
                        f"Jira API error fetching comments for {issue_key}: {comments_response.status_code} - {comments_response.text}"
                    )

                return "\n".join(content_parts)

        except Exception as e:
            self._logger.error(
                f"Error fetching Jira content for {url}: {e}", exc_info=True
            )
            return f"Error fetching Jira content: {e}"

    async def semantic_search(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        query = resp["query"]

        if not query.strip():
            return "Query cannot be empty. Please provide a valid search query."

        await self.send_aci_status(f"Searching for code")

        feature_id = await cache.get("feature_id")
        feature = FeatureEntity.get(feature_id)
        assert feature is not None

        search_vars = await cache.get("search_vars")

        try:
            assert feature.code_graph_id is not None
            graph = GraphRag(code_graph_id=feature.code_graph_id, **search_vars)

            graph_results = await graph.search(
                query,
                overlay_files=await cache.get("modified_files", []),
                only_tests=False,
            )

            results = []
            for n, weight in graph_results:
                # Ignore FILE nodes which have more specific data (i.e. classes/funcs within the file)
                # but allow them if there are only file nodes for that file (e.g. chunked text)
                file_has_other_nodes = any(
                    n2
                    for n2, _ in graph_results
                    if n2.file_name == n.file_name and n2.type != KGNodeType.FILE
                )
                if n.type != KGNodeType.FILE or not file_has_other_nodes:
                    content = await self.file_rpc.read(n.file_name)
                    if not content:
                        continue
                    lines = content.split("\n")
                    if n.line < len(lines):
                        results.append(
                            {
                                "file": n.file_name,
                                "line_number": n.line,
                                "content": lines[n.line][:200],
                                "weight": weight,
                            }
                        )

        except Exception:
            self._logger.exception(f"Error in symbol search, returning empty.")
            return json.dumps([])

        return json.dumps(results)

    async def find_phrase(self, resp: dict[str, Any]) -> str:
        phrase = resp["exact_match_phrase"]
        matches = [
            f"{fn}:{i}|{content[:100]}"
            for fn, i, content in await self.file_rpc.search(
                phrase, overlay_modified=True
            )
        ]

        self._logger.debug(f"FIND PHRASE: {phrase}")

        return "\n".join(matches[:100]) + (
            "\n(limited to first 100 results)" if len(matches) > 100 else ""
        )

    def get_immediate_children(
        self, all_descendants: Iterable[str], current_path: str
    ) -> list[str]:
        """
        Extract immediate children of current_path from a list of all descendant paths.

        Args:
            all_descendants (list): List of strings containing all descendant file/directory paths
            current_path (str): The parent path to find immediate children for

        Returns:
            list: Immediate children paths of the current_path
        """
        # Normalize the current path to ensure consistent handling
        normalized_path = current_path.rstrip("/") + "/"

        children = set()

        for path in all_descendants:
            # Skip if path is the same as current_path
            if path == current_path or path == normalized_path:
                continue

            # Check if this path is under the current_path
            if path.startswith(normalized_path):
                # Get the relative path from current_path
                relative_path = path[len(normalized_path) :]

                # Split on first '/' to get immediate child
                first_segment = relative_path.split("/", 1)[0]

                # If there's content in first_segment, it's an immediate child
                if first_segment:
                    children.add(normalized_path + first_segment)

        return sorted(list(children))

    async def list_files(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        await self.send_aci_status("Listing files in the repository...")

        subpath = resp.get("subpath", "")

        files = await self.file_rpc.list(overlay_modified=True)

        if sum(len(f) for f in files if f.startswith(subpath)) > 100_000:
            out = f"The repository is too large to list all files. Here are the files and folders within '{subpath}':\n"
            out += "\n".join(self.get_immediate_children(files, subpath))
            return out

        files = sorted(f for f in files if f.startswith(subpath))
        return "\n".join(files)

    async def run_single_command(
        self,
        command: str,
        image: Optional[str] = None,
        mount_dir: Optional[str] = None,
        timeout: int = 60,
    ) -> tuple[int, str]:
        self._logger.debug(f"RUNNING SINGLE COMMAND {command}")

        cache = self.cache
        modified_files = await cache.get("output_modified_files", {})

        if await cache.get("headless_command_execution", False):
            feature_id = await cache.get("feature_id")
            feature = FeatureEntity.get(feature_id)
            assert feature is not None
            git_url = await cache.get("repo_url")
            try:
                shell_resp = await self._analysis_client.shell(
                    git_url,
                    feature_id,
                    feature.name,
                    command,
                    overlay_files=modified_files,
                    image=image,
                    mount_dir=mount_dir,
                    timeout=timeout,
                )
            except asyncio.CancelledError:
                raise
            except:
                self._logger.exception("Error in shell execution.")
                return -123, "There was an internal error while running the command."

            # Proto -> WSMessage transform so all the below logic doesn't have to be duplicated
            cmd_resp = RunCommandResponse(
                exit_code=shell_resp.exit_code,
                output=shell_resp.output,
                modified_files=[
                    ChatModifiedFile(
                        name=mf.name,
                        projectPath=mf.project_path,
                        content=mf.content,
                        deleted=mf.deleted,
                    )
                    for mf in shell_resp.modified_files
                ],
            )

        else:
            chat_modified_files: list[ChatModifiedFile] = []
            for fn, content in modified_files.items():
                deleted = content == "BISMUTH_DELETED_FILE"

                if deleted:
                    print("DELETED_FN", fn)

                chat_modified_files.append(
                    ChatModifiedFile(
                        name=pathlib.Path(fn).name,
                        projectPath=fn,
                        content=content,
                        deleted=deleted,
                    )
                )

            await self.send_message_callback(
                WSMessage(
                    type=WSMessageType.RUN_COMMAND,
                    run_command=RunCommandMessage(
                        command=command,
                        output_modified_files=chat_modified_files,
                    ),
                )
            )

            ws_resp = await self.recv_message_callback()
            assert ws_resp.type == WSMessageType.RUN_COMMAND_RESPONSE
            assert ws_resp.run_command_response is not None
            cmd_resp = ws_resp.run_command_response

        for mf in cmd_resp.modified_files:
            modified_files[mf.project_path] = (
                mf.content if not mf.deleted else "BISMUTH_DELETED_FILE"
            )
        await cache.set("output_modified_files", modified_files)

        for mf in cmd_resp.modified_files:
            async with cache.with_suffix(f"file_edit_selection_{mf.project_path}"):
                before = await cache.get("lines", None)
                if before is None:
                    continue
                adjust = len(mf.content.split("\n")) - len(before)
                await cache.set(
                    "lines_below", (await cache.get("lines_below")) + adjust
                )
                await cache.set("lines", mf.content.split("\n"))

        def reduce_cli_output(raw_output):
            """
            Reduces CLI output that contains carriage returns to show only the final visible content of each line.
            """
            lines = raw_output.split("\n")
            result = []

            for line in lines:
                segments = line.split("\r")
                non_empty_segments = [s for s in segments if s.strip()]
                if non_empty_segments:
                    result.append(non_empty_segments[-1])
                else:
                    result.append(line)

            return "\n".join(result)

        output = reduce_cli_output(cmd_resp.output)

        return cmd_resp.exit_code, output

    async def get_test_config(self) -> Optional[BismuthTestTOML]:
        bismuth_toml = await self.file_rpc.read("bismuth.toml", overlay_modified=True)
        if bismuth_toml is None:
            return None
        try:
            config = BismuthTOML.model_validate(tomllib.loads(bismuth_toml))
            return config.test
        except:
            return None

    async def finalize(self, resp: dict[str, Any]) -> str:
        self._logger.debug("Finalizing...")
        cache = self.cache
        open_files = await cache.get("viewer_open_files", [])

        self._attempted_finalize = True

        if (
            self._mode == ACIMode.FULL_SCAN_BUG_FINDER
            and len(await cache.get("current_questions", [])) > 0
            and self.recursion_depth == 0
        ):
            return (
                "You must resolve all questions before finalizing, exploring the codebase to answer all questions thoroughly.\nHere are the current questions:\n"
                + "\n".join(await cache.get("current_questions", []))
            )

        if self._mode == ACIMode.FULL_SCAN_BUG_FINDER_QUESTIONS:
            subsystem_files = await cache.get("subsystem_files")
            looked_at_files = await cache.get("looked_at_files")
            if set(subsystem_files) - set(looked_at_files):
                self._logger.debug(
                    f"Subsystem files: {subsystem_files}, Looked at files: {looked_at_files}"
                )
                (prompt, _, _) = await self.prompt_and_toolset_for_current_mode()
                return (
                    prompt
                    + "\nYou must explore all files in the subsystem before finalizing. These files have not been explored yet:\n"
                    + "\n".join(set(subsystem_files) - set(looked_at_files))
                )

        test_cfg = await self.get_test_config()
        if test_cfg and test_cfg.command:
            await self.send_aci_status("Running test suite...")
            exit_code, test_out = await self.run_single_command(
                test_cfg.command,
                image=test_cfg.image,
                mount_dir=test_cfg.mount_dir,
                timeout=600,
            )
            await trace_output(test_out, "finalize_test_run")
            if exit_code != 0 and exit_code != -123:
                test_out = "\n".join(test_out.split("\n")[-500:])
                return f"Project test suite failed. This exact test command must pass: `{test_cfg.command}`\nRunning it currently fails with exit code: {exit_code}\nOutput:\n```\n{test_out}\n```"
        else:
            self._logger.info("No test config?")

        if self.recursion_depth == 0:
            for i in range(0, len(open_files)):
                open_files[i] = "CLOSED"

            await cache.set("viewer_open_files", open_files)

        self.finalized = True

        self._logger.info("Finalized.")

        raise StopAsyncIteration(f"Finalized.")

    async def update_change_log(self, change: str):
        cache = self.cache
        change_log = await cache.get("change_log", [])
        change_log.append(change)
        await cache.set("change_log", change_log)

    async def close_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["close_file"]["input_schema"]
        )
        if not passed:
            return f"{failing_key} is required for 'close_file', please try again with the correct parameters."
        open_files = await cache.get("viewer_open_files", [])

        file_id = int(resp["file_id"])

        if len(open_files) == 1 and open_files[file_id] == "placeholder_file":
            return "You must create or open at least one new file before closing the placeholder file!"

        if file_id >= len(open_files) or file_id < 0:
            files_with_id = []
            for idx, file in enumerate(open_files):
                files_with_id.append(f"{idx}: {file}")
            return "Invalid file id. Valid files are:\n" + "\n".join(files_with_id)

        self._logger.debug(f"CLOSE {open_files[file_id]}")

        if open_files[file_id] == "CLOSED":
            return "That file is already closed, please take a new action."

        await self.send_message_callback(
            WSMessage(
                type=WSMessageType.ACI,
                aci=ACIMessage(
                    action=ACIMessage.Action.CLOSE,
                    status=f"Closed {open_files[file_id]}",
                ),
            )
        )

        only_placeholder = False

        if len(open_files) == 1 and open_files[file_id] == "placeholder_file":
            only_placeholder = True

        open_files[file_id] = "CLOSED"

        await cache.set("viewer_open_files", open_files)

        if all([file == "CLOSED" for file in open_files]) and not only_placeholder:
            self._logger.debug("ALL FILES CLOSED CALLED FINALIZE WITH TESTS")
            return await self.finalize({})

        try:
            new_active_fn = next((fn for fn in open_files if fn != "CLOSED"))
            return await self.manipulate(
                SwitchAction(file=new_active_fn), new_active_fn
            )
        except StopIteration:
            self._logger.warning(
                "No next file, likely closed placeholder_file first in new project."
            )
            return ""

    async def delete_file(self, resp: dict[str, Any]) -> str:
        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["delete_file"]["input_schema"]
        )
        if not passed:
            return f"{failing_key} is required for 'delete', please try again with the correct parameters."

        file_id = int(resp["file_id"])
        cache = self.cache
        open_files = await cache.get("viewer_open_files", [])
        active_file = await cache.get("active_file")

        if is_url(active_file):
            self._turns_remaining += 1  # Don't penalize
            return "Cannot delete a URL. URLs are read-only."

        target_file_to_delete = (
            open_files[file_id] if file_id < len(open_files) else None
        )
        if target_file_to_delete and is_url(target_file_to_delete):
            self._turns_remaining += 1  # Don't penalize
            return "Cannot delete a URL. URLs are read-only."

        if file_id >= len(open_files) or file_id < 0:
            files_with_id = []
            for idx, file in enumerate(open_files):
                files_with_id.append(f"{idx}: {file}")
            return "Invalid file id. Valid files are:\n" + "\n".join(files_with_id)

        if open_files[file_id] == "CLOSED":
            return "That file is already closed, please take a new action."

        if open_files[file_id] != active_file:
            return "You may only delete the active file, please switch to the file you would like to delete."

        self._logger.debug(f"DELETE {open_files[file_id]}")

        modified_files = await cache.get("output_modified_files", {})

        fn = open_files[file_id]

        modified_files[fn] = "BISMUTH_DELETED_FILE"

        async with cache.with_suffix(f"file_edit_selection_{fn}"):
            await cache.delete("lines_above")
            await cache.delete("lines_below")
            await cache.delete("index")
            await cache.delete("lines")

        await cache.set("output_modified_files", modified_files)

        open_files[file_id] = "CLOSED"

        await self.update_change_log(resp["step"])
        await cache.set("viewer_open_files", open_files)

        await self.send_message_callback(
            WSMessage(
                type=WSMessageType.ACI,
                aci=ACIMessage(
                    action=ACIMessage.Action.CLOSE,
                    status=f"Deleted {fn}",
                ),
            )
        )

        try:
            new_active_fn = next((fn for fn in open_files if fn != "CLOSED"))
        except StopIteration:
            self._logger.warning(
                "No next file, likely deleted placeholder_file first in new project."
            )
            return ""

        session_id = await cache.get("msg_session_id")
        chat_session = ChatSessionEntity.get(session_id)
        assert chat_session is not None
        session_context = chat_session.get_context()
        context_deleted_files = session_context.get("deleted_files", [])
        context_deleted_files.append(fn)

        session_context["deleted_files"] = context_deleted_files
        chat_session.set_context(session_context)

        return await self.manipulate(SwitchAction(file=new_active_fn), new_active_fn)

    async def scroll_down_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["scroll_down_file"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'scroll_down_file', please try again with the correct parameters."

        scroll = int(resp["scroll"])

        self._logger.debug(f"SCROLL DOWN {scroll}")

        active_file = await cache.get("active_file")

        async with cache.with_suffix(f"file_edit_selection_{active_file}"):
            below = await cache.get("lines_below")
            if below == 0:
                return "You are already at the end of the file. Take another action and do not try to scroll down further."

        content = await self.manipulate(ScrollDownAction(scroll=scroll), active_file)

        return content

    async def scroll_up_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["scroll_up_file"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'scroll_up_file', please try again with the correct parameters."

        scroll = int(resp["scroll"])

        self._logger.debug(f"SCROLL UP {scroll}")

        active_file = await cache.get("active_file")

        content = await self.manipulate(ScrollUpAction(scroll=scroll), active_file)

        return content

    async def create_file(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["create_file"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'create_file', please try again with the correct parameters."

        file = self.normalize_filename(resp["file"])
        if is_url(file):
            self._turns_remaining += 1  # Don't penalize
            return "Cannot create a URL. URLs are read-only resources."

        content = resp["content"]
        step = resp["step"]
        self._logger.debug(f"CREATE FILE {file}")

        exists = (
            await self.file_rpc.read(file, overlay_modified=True) is not None
        ) or (
            (await cache.get("output_modified_files", {})).get(
                file, "BISMUTH_DELETED_FILE"
            )
            != "BISMUTH_DELETED_FILE"
        )
        if exists:
            return "That file exists already, try jumping to a definition in it to open it."

        content = await self.manipulate(CreateAction(content=content, file=file), file)

        session_id = await cache.get("msg_session_id")
        chat_session = ChatSessionEntity.get(session_id)
        assert chat_session is not None
        session_context = chat_session.get_context()
        context_created_files = session_context.get("created_files", [])
        context_created_files.append(file)

        session_context["created_files"] = context_created_files
        chat_session.set_context(session_context)
        await self.update_change_log(step)

        return content

    async def go_to_line(self, resp: dict[str, Any]) -> str:
        self._logger.debug("GO TO LINE")

        cache = self.cache
        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["go_to_line"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'go_to_line', please try again with the correct parameters."

        active_file = await cache.get("active_file")

        line = int(resp["line_num"])

        content = await self.manipulate(JumpAction(line=line), active_file)

        return content

    async def switch_to_driver_mode(self, resp: dict[str, Any]) -> None:
        self._logger.debug("SWITCHING TO DRIVER")
        await self.send_aci_status("Planning next step...")
        await self._switch_to_mode(ACIMode.DRIVER, resp["results"])

    async def switch_to_navigation_mode(self, resp: dict[str, Any]) -> None:
        self._logger.debug("SWITCHING TO NAVIGATOR")
        await self.send_aci_status("Planning next step...")
        await self._switch_to_mode(ACIMode.NAVIGATE, resp["goal"])

    async def switch_to_debug_mode(self, resp: dict[str, Any]) -> None:
        self._logger.debug("SWITCHING TO DEBUGGER")
        await self.send_aci_status("Planning next step...")
        await self._switch_to_mode(ACIMode.DEBUG, resp["goal"])

    async def switch_to_editing_mode(self, resp: dict[str, Any]) -> None:
        self._logger.debug("SWITCHING TO EDITOR")
        await self.send_aci_status("Planning next step...")
        await self._switch_to_mode(ACIMode.EDIT, resp["goal"])

    async def _switch_to_mode(self, mode: ACIMode, communication) -> None:
        self._logger.debug(
            f"Switching to mode {mode} with communication {communication}"
        )
        self._mode = mode
        await self.cache.set("driver_subsystem_communications", communication)

    async def get_customization(self) -> str:
        customization = (await self.file_rpc.read("AGENTS.md")) or ""
        if self._mode in (
            ACIMode.FULL_SCAN_BUG_FINDER_QUESTIONS,
            ACIMode.FULL_SCAN_BUG_FINDER,
            ACIMode.CI_BUG_FINDER_QUESTIONS,
            ACIMode.CI_BUG_FINDER,
        ):
            bismuth_review = (await self.file_rpc.read("REVIEW_AGENT.md")) or ""
            if customization and bismuth_review:
                customization += "\n\n"
            customization += bismuth_review

        if not customization:
            return ""

        return (
            "<customization>\nHere is some additional information and requirements from the code base you are working with. "
            "Always assume everything listed here is true, and follow any guidance given about how to work with the code base - "
            "e.g. project structure, standard practices, coding conventions, testing protocols, etc.\n\n"
            + customization
            + "\n</customization>"
        )

    async def prompt_and_toolset_for_current_mode(self):
        if self.mode.value == ACIExecutionMode.SINGLE.value:
            mode = ACIMode.CONSTRAINED
        else:
            mode = self._mode

        input_task = self._input_task

        if self._recursive_task:
            input_task = self._recursive_task

        task_config = await self.cache.get("planned_task_config", {})
        prompt_extra = await self.cache.get("prompt_extra", {})
        starting_context = await self.cache.get("unmodified_context", {})

        pinned_file_context = ""

        if not bool(self._pinned_files):
            for fn, content in self._pinned_files.items():
                tmp = textwrap.dedent(
                    f"""
                <file name="{fn}">
                {content}
                </file>\n
                """
                )

                pinned_file_context += tmp

        toolset = (await self.toolsets())[mode]

        files = "\n".join(list(starting_context.keys()))
        prompt = self.prompts()[mode]

        # Add gasp format instructions FIRST if gasp tools are enabled
        if USE_GASP_TOOLS:
            try:
                from gasp.template_helpers import interpolate_prompt
                from daneel.executors.aci.aci_tool_models import (
                    MODE_TOOL_UNIONS,
                    MODE_TOOL_UNIONS_WITH_HUMAN,
                )

                # Get the appropriate tool union for this mode
                if self._humanlayer_enabled and mode == ACIMode.DRIVER:
                    tool_union = MODE_TOOL_UNIONS_WITH_HUMAN[mode]
                else:
                    tool_union = MODE_TOOL_UNIONS[mode]

                # Interpolate the prompt with the tool format
                prompt = interpolate_prompt(
                    prompt, tool_union, format_tag="return_type"
                )
            except ImportError:
                self._logger.warning("Failed to import gasp for prompt interpolation")
            except Exception as e:
                self._logger.warning(f"Failed to interpolate prompt: {e}")

        viewer_history = await self.cache.get("viewer_history", [])
        prompt = Template(prompt).render(
            lines=self._lines_in_view(),
            task=input_task,
            files=files,
            viewer_state=viewer_history[0],
            turns=self.initial_turns,
            execution_mode=self.mode,
            pinned_files=pinned_file_context,
            customization=await self.get_customization(),
            current_date=datetime.datetime.now().strftime("%B %d, %Y"),  # July 14, 2025
            **task_config,
            **prompt_extra,
        )

        return (prompt, toolset, mode)

    async def go_to_def(self, resp: dict[str, Any]) -> str:
        cache = self.cache
        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["go_to_def"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'go_to_def', please try again with the correct parameters."

        active_file = await cache.get("active_file")

        symbol = resp["symbol"]
        line = int(resp["line"])
        self._logger.debug(f"GO TO DEF {symbol} {line}")
        current_contents = await self.file_rpc.read(active_file, overlay_modified=True)
        assert current_contents is not None
        if current_contents.count("\n") < line:
            return f"The line {line} is out of bounds for the file {active_file}."

        col = current_contents.split("\n")[line].find(symbol)
        if col == -1:
            return f"The symbol {symbol} was not found on line {line}."

        git_url = await cache.get("repo_url")
        feature_id = await cache.get("feature_id")
        feature = FeatureEntity.get(feature_id)
        assert feature is not None

        try:
            res = await self._analysis_client.go_to_def(
                git_url,
                feature_id,
                feature.name,
                active_file,
                line
                + 1,  # display line numbers are 0-indexed, lsp interface is 1-indexed
                col,
                await cache.get("output_modified_files", {}),
            )
        except asyncio.CancelledError:
            raise
        except Exception:
            self._logger.warning("Exception in go_to_def", exc_info=True)
            res = None

        self._logger.debug(f"GO TO DEF: {symbol} = {res}")
        if res is None or not res.HasField("definition"):
            return "No definition found."

        def_file = res.definition.filename
        def_line = res.definition.line
        def_line -= 1

        await cache.set(
            "go_to_def_stack",
            (await cache.get("go_to_def_stack", [])) + [(symbol, active_file, line)],
        )

        open_files = await cache.get("viewer_open_files", [])
        if def_file not in open_files:
            open_files.append(def_file)
            await cache.set("viewer_open_files", open_files)

            def_file_contents = await self.file_rpc.read(def_file)
            assert def_file_contents is not None
            lines = def_file_contents.split("\n")

            # These will be immediately overwritten by the next manipulate call
            index = min(def_line + self._lines_in_view(), len(lines))
            lines_above = def_line
            lines_below = len(lines) - index

            async with cache.with_suffix(f"file_edit_selection_{def_file}"):
                await cache.set("lines_above", lines_above)
                await cache.set("lines_below", lines_below)
                await cache.set("index", index)
                await cache.set("lines", lines)

        content = await self.manipulate(JumpAction(line=def_line), def_file)

        return content

    async def pop_go_to_def(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        stack = await cache.get("go_to_def_stack", [])

        if not stack:
            return "No more definitions to pop."

        _, file, line = stack.pop()

        await cache.set("go_to_def_stack", stack)

        open_files = await cache.get("viewer_open_files", [])
        if file not in open_files:
            open_files.append(file)
            await cache.set("viewer_open_files", open_files)

            lines = (await self.file_rpc.read(file)).split("\n")  # type: ignore

            index = min(line + self._lines_in_view(), len(lines))
            lines_above = line
            lines_below = len(lines) - index

            async with cache.with_suffix(f"file_edit_selection_{file}"):
                await cache.set("lines_above", lines_above)
                await cache.set("lines_below", lines_below)
                await cache.set("index", index)
                await cache.set("lines", lines)

        return await self.manipulate(JumpAction(line=line), file)

    async def find_references(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["find_references"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'find_references', please try again with the correct parameters."

        active_file = await cache.get("active_file")

        symbol = resp["symbol"]
        line = int(resp["line"])
        current_contents = await self.file_rpc.read(active_file, overlay_modified=True)
        assert current_contents is not None
        if current_contents.count("\n") < line:
            return f"The line {line} is out of bounds for the file {active_file}."

        col = current_contents.split("\n")[line].find(symbol)
        if col == -1:
            return f"The symbol {symbol} was not found on line {line}."

        self._logger.debug(f"FIND REFS: {symbol}")

        git_url = await cache.get("repo_url")
        feature_id = await cache.get("feature_id")
        feature = FeatureEntity.get(feature_id)
        assert feature is not None

        try:
            res = list(
                (
                    await self._analysis_client.symbol_refs(
                        git_url,
                        feature_id,
                        feature.name,
                        active_file,
                        line
                        + 1,  # display line numbers are 0-indexed, lsp interface is 1-indexed
                        col,
                        await cache.get("output_modified_files", {}),
                    )
                ).references
            )
        except asyncio.CancelledError:
            raise
        except Exception:
            self._logger.warning("Exception in find_references", exc_info=True)
            res = []

        return "\n".join(f"{ref.filename}:{ref.line}" for ref in res)

    async def reach_out_to_human_for_assistance(self, resp: dict[str, Any]) -> str:
        self._logger.debug("REACHING OUT TO HUMAN")

        message = resp["message"]

        from humanlayer import HumanLayer

        hl = HumanLayer()
        tool = hl.human_as_tool()

        hr = tool(message)

        return hr

    async def analyze_system(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["analyze_system"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'analyze_system', please try again with the correct parameters."

        active_file = await cache.get("active_file")

        await cache.set("test_failure_loop_count", 0)

        test_output = await cache.get("test_output")

        # test_output = test_output.split("<system_message>")[0]

        await cache.set("test_output", test_output)

        executor = self.tool_executors["analyze_system"]
        assert isinstance(executor, ACISystemAnalysisExecutor)

        semaphore = Semaphore()

        exec_response = await executor.process(cache, semaphore, focus=resp["focus"])

        if exec_response["status"] == "success":
            analysis = exec_response["result"]
        else:
            analysis = "Analysis failed, you're flying blind. Good luck!"

        # Add analysis to current viewer state
        content = await self.manipulate(
            SystemAnalysisAction(system_analysis_output=analysis), active_file
        )

        return content

    async def run_command(self, resp: dict[str, Any]) -> str:
        self._logger.debug(f"RUNNING COMMAND {resp['command']}")
        await self.send_aci_status(f"Running commands")

        if self.send_message_callback == null_send_callback:
            return "Running commands is not supported in this environment."

        command = resp["command"]
        test_cfg = await self.get_test_config()
        if test_cfg:
            image = test_cfg.image
            mount_dir = test_cfg.mount_dir
        else:
            image = None
            mount_dir = None
        exit_code, output = await self.run_single_command(
            command, image=image, mount_dir=mount_dir
        )

        out = f"exit code: {exit_code}\noutput:\n"
        out += "```\n"
        out += "\n".join(output.split("\n")[-500:])
        out += "\n```"
        if len(output.split("\n")) > 500:
            out += "\n(Output truncated to last 500 lines.)"

        return out

    async def report_bug_ci(self, resp: dict[str, Any]) -> str:
        self._logger.debug("REPORTING BUG")

        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["report_bug_ci"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'report_bug', please try again with the correct parameters."

        file = self.normalize_filename(resp["file"])
        lines_to_replace = resp["lines_to_replace"].rstrip()
        replace_text = resp["replace_text"].rstrip()
        description = resp["bug_description"]
        grounding = resp.get("grounding", None)

        file_content = await self.file_rpc.read(file, overlay_modified=True)
        if file_content is None:
            return "The file you specified does not exist."

        if lines_to_replace not in file_content:
            return "The lines you specified to replace are not in the file."
        start_line = (
            file_content[: file_content.index(lines_to_replace)].count("\n") + 1
        )
        end_line = start_line + lines_to_replace.count("\n")

        await self.send_aci_status(f"Fixing a bug in {file}")

        issues = await cache.get("identified_bugs", [])
        for issue in issues:
            if issue["file"] == file and issue["start_line"] == start_line:
                return "An issue has already been reported on this line of code. If there are no other bugs to report, call finalize."

        if grounding is not None:
            if grounding == "fuzzing":
                grounding = {
                    "source": "fuzzing",
                    "code": await cache.get("fuzzing_code", None),
                }
            else:
                grounding = {"source": "test", "test": grounding}

        issues.append(
            {
                "file": file,
                "start_line": start_line,
                "end_line": end_line,
                "description": description,
                "suggested_fix": replace_text,
                "grounding": grounding,
            }
        )
        await cache.set("identified_bugs", issues)

        await self.update_change_log(description)

        if self._apply_bug_fixes and replace_text:
            active_file = await cache.get("active_file")

            if active_file != file:
                await self.switch_file(resp)

            try:
                action = EditAction(
                    lines_to_replace=lines_to_replace,
                    replace_text=replace_text,
                    file=file,
                )
                return await self.manipulate(action, active_file)
            except ValueError as e:
                self._logger.exception("Error editing file")
                return str(e)

        if len(issues) >= 10:
            self._logger.info("Bailing after 10 issues reported")
            raise StopAsyncIteration()

        if self._apply_bug_fixes:
            return "Bug reported and your change has been applied. Continue to report other potential issues, or call finalize if done."
        else:
            return "Bug reported. Your change is only a suggestion and has not been applied, so be sure to not report this same bug again. Continue to report other potential issues, or call finalize if done."

    async def add_question(self, resp: dict[str, Any]) -> str:
        self._logger.debug("ADDING QUESTION")

        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["add_question"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'add_question', please try again with the correct parameters."

        new_questions = resp["questions"]
        if not isinstance(new_questions, list):
            new_questions = [new_questions]
        self._logger.debug(f"ADDING QUESTIONS {new_questions}")

        for q in new_questions:
            await self._question_out_queue.put(q)

        questions = await cache.get("current_questions", [])
        questions.extend(new_questions)
        await cache.set("current_questions", questions)

        await trace_output(f"Questions added: {new_questions}", "explore_thoughts")

        return "Questions added. Continue to explore the codebase."

    async def resolve_question(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["resolve_question"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'resolve_question', please try again with the correct parameters."

        question_id = resp["question_id"]
        answer = resp["answer"]

        questions = await cache.get("current_questions", [])

        if question_id >= len(questions) or question_id < 0:
            return (
                "Invalid question id. Please provide a valid question id. Current questions:\n"
                + "\n".join(f"{i}: {q}" for i, q in enumerate(questions))
            )

        self._logger.debug(
            f"resolve question {question_id} ({questions[question_id]}): {answer}"
        )

        await trace_output(
            f"resolve question {question_id} ({questions[question_id]}): {answer}",
            "explore_thoughts",
        )

        questions = questions[:question_id] + questions[question_id + 1 :]
        await cache.set("current_questions", questions)

        if questions:
            if self._last_shown_viewer_state < self._step_count - 10:
                file = await cache.get("active_file")
                await self.manipulate(SwitchAction(file=file), file)
                return (
                    "Question resolved. Remember to navigate around the code base to answer your questions. Here is the current viewer state:\n"
                    + await self.manipulate(
                        SwitchAction(file=file),
                        file,
                    )
                )
            else:
                return "Question resolved. Current questions:\n" + "\n".join(
                    f"{i}: {q}" for i, q in enumerate(questions)
                )
        else:
            raise StopAsyncIteration("All questions resolved.")

    async def add_memory(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["add_memory"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'add_memory', please try again with the correct parameters."

        memory = resp["memory"]
        self._logger.debug(f"ADDING MEMORY {memory}")
        await self.send_aci_status("Adding memory")

        memories = await cache.get("bug_finding_memories", [])
        memories.append(memory)
        await cache.set("bug_finding_memories", memories)

        await trace_output(f"Memory added: {memory}", "explore_thoughts")

        return "Memory added. Continue to explore the codebase."

    async def bug_recurse_edit(self, resp: dict[str, Any]) -> str:
        """
        "Specialization" of recurse for full scan bug finding. Drops into edit mode instead of driver, and accumulates each call of this into a list of commits.
        """
        subtask = resp["subtask"]
        cache = self.cache

        if "judge" in self.tool_executors:
            judge = self.tool_executors["judge"]
            if not await judge.process(cache, Semaphore(), subtask=subtask):  # type: ignore
                self._logger.info("Judge rejected subtask")
                return "This task is not something that should be done. Remember that you should not perform very large refactorings or changes to the codebase, only fix logic errors, obviously incorrect behavior, or user-facing security vulnerabilities."

        self.recursion_depth += 1
        self._recursive_task = subtask

        if self.recursion_depth > 1:
            self._logger.info("AGENT RECURSION LIMIT REACHED, SKIPPING RECURSE")
            self.recursion_depth -= 1

            return "Recursion depth limit has been reached please try completing the task by yourself."

        self._logger.info(f"GOING RECURSIVE {subtask}")
        await self.send_aci_status("Creating a PR for a bug")

        executor = self.tool_executors["recurse"]

        # NOP Switch to force recursive subtask into viewer state
        file = await cache.get("active_file")

        await self.manipulate(SwitchAction(file=file), file)

        cur_mode = self._mode
        await self._switch_to_mode(ACIMode.EDIT, subtask)

        old_changelog = await cache.get("change_log", [])

        try:
            _response = await executor.process(  # type: ignore
                self.cache,
                Semaphore(),
                subtask=subtask
                + ". DO NOT write tests or attempt to build the project, only edit the code.",
            )
        except asyncio.CancelledError:
            raise
        except Exception:
            self._logger.warning(
                "Error running recursive bug fix edit task", exc_info=True
            )

        modified_files = await cache.get("output_modified_files", {})
        await cache.set(
            "commits",
            await cache.get("commits")
            + [{"message": subtask, "files": modified_files}],
        )

        new_changelog = await cache.get("change_log", [])

        self.recursion_depth -= 1
        self._recursive_task = None
        self.finalized = False
        self._attempted_finalize = False
        self._mode = cur_mode

        # Reset viewer_state to have the non recursive task in its most recent state
        file = await cache.get("active_file")

        await self.manipulate(SwitchAction(file=file), file)

        return (
            "Edit task completed. Here are the changes:\n"
            + "\n".join(new_changelog[len(old_changelog) :])
            + "\n\nContinue to explore the codebase."
        )

    async def bug_recurse_fuzz(self, resp: dict[str, Any]) -> str:
        target = resp["target"]
        cache = self.cache

        self.recursion_depth += 1

        if self.recursion_depth > 1:
            self._logger.info("AGENT RECURSION LIMIT REACHED, SKIPPING RECURSE")
            self.recursion_depth -= 1

            return "Recursion depth limit has been reached please try completing the task by yourself."

        self._logger.info(f"GOING RECURSIVE FUZZ {target}")

        viewer_history = await cache.get("viewer_history", [])
        await cache.set("viewer_history", [])

        executor = self.tool_executors["fuzz"]

        cur_mode = self._mode
        await self._switch_to_mode(ACIMode.FUZZ_GEN, target)

        try:
            result = await executor.process(  # type: ignore
                self.cache,
                Semaphore(),
                target_symbol=target,
            )
        except asyncio.CancelledError:
            raise
        except Exception:
            self._logger.warning("Error running fuzzer executor", exc_info=True)
            result = {"status": "error"}

        self.recursion_depth -= 1
        self._recursive_task = None
        self.finalized = False
        self._attempted_finalize = False
        self._mode = cur_mode

        await cache.set("viewer_history", viewer_history)

        # Reset viewer_state to have the non recursive task in its most recent state
        file = await cache.get("active_file")

        await self.manipulate(SwitchAction(file=file), file)

        if result.get("status") == "error":
            return "An error occurred while trying to run the fuzzer: " + result.get(
                "message", "unknown internal failure"
            )

        return (
            "Fuzzing completed. Here is the run output:\n\n"
            + result.get("result", "")
            + "\nIf this appears to show a valid bug, report and fix it. Specifically mention that it was found by fuzzing."
        )

    async def run_fuzzer(self, resp: dict[str, Any]) -> str:
        cache = self.cache

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["run_fuzzer"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'run_fuzzer', please try again with the correct parameters."

        await self.send_aci_status("Running a fuzzer")

        fuzzer = resp["code"]
        await cache.set("fuzz_code", fuzzer)

        feature_id = await cache.get("feature_id")
        feature = FeatureEntity.get(feature_id)
        assert feature is not None
        git_url = await cache.get("repo_url")

        try:
            fuzz_resp = await self._analysis_client.fuzz(
                git_url,
                feature_id,
                feature.name,
                fuzzer,
                self.normalize_filename(resp["filename"]),
            )
        except asyncio.CancelledError:
            raise
        except Exception:
            logging.exception("Error running fuzzer")
            return "An internal error occurred while trying to run the fuzzer. Call finalize and do not attempt to fuzz again."

        active_file = await cache.get("active_file")
        content = await self.manipulate(
            TestAction(test_output=fuzz_resp.output), active_file
        )

        await cache.set("fuzz_output", fuzz_resp.output)

        return content

    async def think(self, resp: dict[str, Any]) -> str:
        self._logger.debug("THINKING")

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["think"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'think', please try again with the correct parameters."

        thoughts = resp["thoughts"]

        if isinstance(thoughts, str):
            thoughts = [thoughts]

        self._logger.debug(f"THINKING {thoughts}")

        return "<thoughts>\n" + "\n".join(thoughts) + "\n</thoughts>\n"

    async def read_file_diff(self, resp: dict[str, Any]) -> str:
        self._logger.debug("READING FILE DIFF")

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["read_file_diff"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'read_file_diff', please try again with the correct parameters."

        file = self.normalize_filename(resp["file"])

        open_files = await self.cache.get("viewer_open_files", [])

        content = await self.file_rpc.read(file, overlay_modified=True)
        if content is None:
            return f"The file {file} does not exist."

        if len(content) > 300_000:
            return "File too big to be shown. Proceed to another file."

        if file in open_files:
            await self.manipulate(SwitchAction(file=file), file)
        else:
            await self.manipulate(OpenAction(content=content, file=file), file)

        diff = (await self.cache.get("file_patches", {})).get(file, "")

        return f'<file name="{file}">\n{content}\n</file>\n\n<diff>\n{diff}\n</diff>'

    async def search_docs(self, resp: dict[str, Any]) -> str:
        self._logger.debug("SEARCHING DOCS")

        passed, failing_key = self.validate_llm_call(
            resp, self.tool_schemas["search_docs"]["input_schema"]
        )

        if not passed:
            return f"{failing_key} is required for 'search_docs', please try again with the correct parameters."

        query = resp["query"]

        results = CitationEntity.search(query, top=10)
        return (
            "\n".join(
                f'<doc_result id={result.id} url="{result.url}">\n{result.content}\n</doc_result>'
                for result in results
            )
            or "No documentation found for the given query."
        )

----------- OLD ASIMOV INFERENCE CLIENTS BUT DECENT EXAMPLE OF HOW WE WERE THINKING BACK THEN --------------

import asyncio
from collections.abc import Hashable
import botocore.exceptions
from pydantic import Field
import json
from typing import Awaitable, Callable, List, Dict, Any, Optional, Tuple, AsyncGenerator
from enum import Enum
from abc import ABC, abstractmethod
import logging

import aioboto3
import httpx
import backoff
import opentelemetry.instrumentation.httpx
import opentelemetry.trace
import pydantic_core
import google.auth
import google.auth.transport.requests
from google import genai
from google.genai import types


from asimov.asimov_base import AsimovBase
from asimov.utils.token_counter import approx_tokens_from_serialized_messages
from asimov.graph import NonRetryableException

logger = logging.getLogger(__name__)
tracer = opentelemetry.trace.get_tracer(__name__)
opentelemetry.instrumentation.httpx.HTTPXClientInstrumentor().instrument()


class InferenceException(Exception):
    """
    A generic exception for inference errors.
    Should be safe to retry.
    ValueError is raised if the request is un-retryable (e.g. parameters are malformed).
    """

    pass


class ContextLengthExceeded(InferenceException, ValueError):
    pass


class RetriesExceeded(InferenceException):
    """
    Raised when the maximum number of retries is exceeded.
    """

    pass


class ChatRole(Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL_RESULT = "tool_result"


class ChatMessage(AsimovBase):
    role: ChatRole
    content: str
    cache_marker: bool = False
    model_families: List[str] = Field(default_factory=list)


class AnthropicRequest(AsimovBase):
    anthropic_version: str
    system: str
    messages: List[Dict[str, Any]]
    tools: List[Dict[str, Any]] = Field(default_factory=list)
    tool_choice: Dict[str, Any] = Field(default_factory=dict)
    max_tokens: int = 512
    temperature: float = 0.5
    top_p: float = 0.9
    top_k: int = 50


class InferenceCost(AsimovBase):
    input_tokens: int = 0
    output_tokens: int = 0
    cache_read_input_tokens: int = 0
    cache_write_input_tokens: int = 0
    # Dollar amount that should be added on to the cost of the request
    dollar_adjust: float = 0.0


class InferenceClient(ABC):
    model: str
    _trace_id: int = 0
    trace_cb: Optional[
        Callable[
            [int, list[dict[str, Any]], list[dict[str, Any]], InferenceCost],
            Awaitable[None],
        ]
    ] = None
    _cost: InferenceCost
    _last_cost: InferenceCost

    def __init__(self):
        self._cost = InferenceCost()
        self._last_cost = InferenceCost()

    async def _trace(self, request, response):
        opentelemetry.trace.get_current_span().set_attribute(
            "inference.usage.input_tokens", self._cost.input_tokens
        )
        opentelemetry.trace.get_current_span().set_attribute(
            "inference.usage.cached_input_tokens",
            self._cost.cache_read_input_tokens,
        )
        opentelemetry.trace.get_current_span().set_attribute(
            "inference.usage.output_tokens", self._cost.output_tokens
        )

        if self.trace_cb:
            logger.debug(f"Request {self._trace_id} cost {self._cost}")
            await self.trace_cb(self._trace_id, request, response, self._cost)
            self._last_cost = self._cost
            self._cost = InferenceCost()
            self._trace_id += 1

    @abstractmethod
    def connect_and_listen(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=0.9, temperature=0.5
    ) -> AsyncGenerator[str, None]:
        pass

    @abstractmethod
    async def get_generation(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=0.9, temperature=0.5
    ):
        pass

    @abstractmethod
    async def _tool_chain_stream(
        self,
        serialized_messages: List[Dict[str, Any]],
        tools: List[Tuple[Callable, Dict[str, Any]]],
        system: Optional[str] = None,
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        tool_choice="any",
        middlewares: List[Callable[[dict[str, Any]], Awaitable[None]]] = [],
    ) -> List[Dict[str, Any]]:
        pass

    @tracer.start_as_current_span(name="InferenceClient.tool_chain")
    async def tool_chain(
        self,
        messages: List[ChatMessage],
        tools: List[Tuple[Callable, Dict[str, Any]]] = [],
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        max_iterations=10,
        tool_choice="any",
        middlewares: List[Callable[[dict[str, Any]], Awaitable[None]]] = [],
        mode_swap_callback: Optional[
            Callable[
                [],
                Awaitable[tuple[str, List[Tuple[Callable, Dict[str, Any]]], Hashable]],
            ]
        ] = None,
        fifo_ratio: Optional[float] = None,
        tool_parser: Optional[
            Callable[[str, Hashable], Awaitable[list[dict[str, Any]]]]
        ] = None,
        tool_result_reducer: Optional[Callable[[list, list], str]] = None,
    ):
        mode = None
        if mode_swap_callback:
            _, _, mode = await mode_swap_callback()

        if tool_parser and not tool_result_reducer:
            raise ValueError(
                "If tool_parser is set then tool_result_reducer must be set."
            )

        last_mode_cached_message: dict[Hashable, int] = {}

        tool_funcs = {tool[1]["name"]: tool[0] for tool in tools}

        system = None
        if messages[0].role == ChatRole.SYSTEM and not tool_parser:
            system = messages[0].content
            messages = messages[1:]

        serialized_messages: list[dict[str, Any]] = [
            {"role": msg.role.value, "content": [{"type": "text", "text": msg.content}]}
            for msg in messages
        ]

        for _ in range(max_iterations):
            for msg in serialized_messages:
                msg["content"][-1].pop("cache_control", None)

            for _, tool in tools:
                tool.pop("cache_control", None)

            if mode_swap_callback and len(serialized_messages) > 2:
                prompt, tools, mode = await mode_swap_callback()

                if not tool_parser:
                    tools[-1][1]["cache_control"] = {"type": "ephemeral"}

                tool_funcs = {tool[1]["name"]: tool[0] for tool in tools}

                if prompt:
                    serialized_messages[0]["content"] = [
                        {"type": "text", "text": prompt}
                    ]

            if mode in last_mode_cached_message:
                serialized_messages[last_mode_cached_message[mode]]["content"][-1][
                    "cache_control"
                ] = {"type": "ephemeral"}

            serialized_messages[-1]["content"][-1]["cache_control"] = {
                "type": "ephemeral"
            }

            last_mode_cached_message[mode] = len(serialized_messages) - 1

            if (
                fifo_ratio
                and (
                    (
                        self._last_cost.cache_read_input_tokens
                        + self._last_cost.input_tokens
                    )
                    / 200000
                )
                > fifo_ratio
            ):
                logger.info(f"cache threshold hit, tossing early messages and retrying")
                # If we hit context length, remove a handful of assistant,user message pairs from the middle
                # A handful so that we can hopefully get at least a couple cache hits with this
                # truncated history before having to drop messages again.

                # We want the earliest thing we remove to be an assistant message (requesting the next tool call), which have odd indices
                start_remove = int(len(serialized_messages) / 3)
                if start_remove % 2 != 1:
                    start_remove += 1
                # And the last thing we remove should be a user message (with tool response), which have even indices
                end_remove = int(len(serialized_messages) * 2 / 3)
                if end_remove % 2 != 0:
                    end_remove -= 1
                logger.debug(
                    f"Removing messages {start_remove} through {end_remove} from serialized messages"
                )
                end_remove += 1  # inclusive
                serialized_messages = (
                    serialized_messages[:start_remove]
                    + serialized_messages[end_remove:]
                )
                for mode in last_mode_cached_message.keys():
                    # Delete markers if they are in the removed range
                    if start_remove <= last_mode_cached_message[mode] < end_remove:
                        del last_mode_cached_message[mode]
                    # And adjust indices of anything that got "slid" back
                    elif last_mode_cached_message[mode] >= end_remove:
                        last_mode_cached_message[mode] -= end_remove - start_remove

            for retry in range(1, 5):
                try:
                    if not tool_parser:
                        resp = await self._tool_chain_stream(
                            serialized_messages,
                            tools,
                            system=system,
                            max_tokens=max_tokens,
                            top_p=top_p,
                            temperature=temperature,
                            tool_choice=tool_choice,
                            middlewares=middlewares,
                        )

                        if not resp:
                            raise InferenceException("no response blocks returned")

                        calls = [c for c in resp if c["type"] == "tool_use"]

                        await self._trace(serialized_messages, resp)
                    else:
                        buf = ""
                        calls = None

                        # this is a little dumb since we serialize them above, but we do the caching updates so whatever
                        unserialized_messages = [
                            ChatMessage(
                                role=ChatRole(msg["role"]),
                                content=msg["content"][0]["text"],
                                cache_marker=msg["content"][0]
                                .get("cache_control", {})
                                .get("type", None)
                                == "ephemeral",
                            )
                            for msg in serialized_messages
                        ]

                        async for token in self.connect_and_listen(
                            unserialized_messages,
                            max_tokens=max_tokens,
                            top_p=top_p,
                            temperature=temperature,
                        ):
                            buf += token

                            calls = await tool_parser(buf, mode)

                        if type(calls) is not list:
                            calls = [calls]  # type: ignore

                        resp = [{"type": "text", "text": buf}]

                    break
                except ContextLengthExceeded as e:
                    logger.info("Non-retryable exception hit (context length), bailing")
                    return serialized_messages
                except NonRetryableException as e:
                    logger.info(f"Non-retryable exception hit ({e}), bailing")
                    raise
                except ValueError as e:
                    logger.info(f"ValueError hit ({e}), bailing")
                    import traceback

                    traceback.print_exc()
                    return serialized_messages
                except InferenceException as e:
                    logger.info("inference exception %s", e, exc_info=True)
                    await asyncio.sleep(3**retry)
                    if retry > 3:
                        # Modify messages to try and cache bust in case we have a poison message or similar
                        serialized_messages[0]["content"][0][
                            "text"
                        ] += "\n\nTFJeD9K6smAnr6sUcllj"
                    continue
                except Exception:
                    logger.warning("generic inference exception", exc_info=True)
                    await asyncio.sleep(3**retry)
                    continue
            else:
                logger.info("Retries exceeded, bailing!")
                raise RetriesExceeded()

            serialized_messages.append(
                {
                    "role": "assistant",
                    "content": resp,
                }
            )

            if not calls:
                serialized_messages.append(
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "You did not correctly make a tool call. Please make a tool call from your available tools.",
                            }
                        ],
                    }
                )
                continue

            content_blocks = []
            for call in calls:
                try:
                    func = tool_funcs.get(call["name"])

                    if not func:
                        result = "This tool is not available please select and use an available tool."
                    else:
                        result = await func(call["input"])
                except StopAsyncIteration:
                    return serialized_messages

                if tool_parser:
                    content_blocks.append(
                        {
                            "type": "text",
                            "content": str(result),
                        }
                    )
                else:
                    content_blocks.append(
                        {
                            "type": "tool_result",
                            "tool_use_id": call["id"],
                            "content": str(result),
                        }
                    )

            if tool_parser and tool_result_reducer:
                content_blocks = [
                    {"type": "text", "text": tool_result_reducer(calls, content_blocks)}
                ]

            serialized_messages.append(
                {
                    "role": "user",
                    "content": content_blocks,
                }
            )
        return serialized_messages


class BedrockInferenceClient(InferenceClient):
    def __init__(self, model: str, region_name="us-east-1"):
        super().__init__()
        self.model = model
        self.region_name = region_name
        self.session = aioboto3.Session()
        self.anthropic_version = "bedrock-2023-05-31"

    @tracer.start_as_current_span(name="BedrockInferenceClient.get_generation")
    @backoff.on_exception(backoff.expo, InferenceException, max_time=60)
    async def get_generation(
        self,
        messages: List[ChatMessage],
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
    ):

        system = ""
        if messages[0].role == ChatRole.SYSTEM:
            system = messages[0].content
            messages = messages[1:]

        request = AnthropicRequest(
            anthropic_version=self.anthropic_version,
            system=system,
            top_p=top_p,
            temperature=temperature,
            max_tokens=max_tokens,
            messages=[
                {
                    "role": msg.role.value,
                    "content": [{"type": "text", "text": msg.content}],
                }
                for msg in messages
            ],
        )

        async with self.session.client(
            service_name="bedrock-runtime",
            region_name=self.region_name,
        ) as client:
            try:
                response = await client.invoke_model(
                    body=request.model_dump_json(exclude={"tools", "tool_choice"}),
                    modelId=self.model,
                    contentType="application/json",
                    accept="application/json",
                )
            except client.exceptions.ValidationException as e:
                raise ValueError(str(e))
            except botocore.exceptions.ClientError as e:
                raise InferenceException(str(e))

            body: dict = json.loads(await response["body"].read())

            self._cost.input_tokens += body["usage"]["input_tokens"]
            self._cost.output_tokens += body["usage"]["output_tokens"]

            await self._trace(request.messages, body["content"])

            return body["content"][0]["text"]

    @tracer.start_as_current_span(name="BedrockInferenceClient.connect_and_listen")
    async def connect_and_listen(
        self,
        messages: List[ChatMessage],
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
    ):
        system = ""
        if messages[0].role == ChatRole.SYSTEM:
            system = messages[0].content
            messages = messages[1:]

        request = AnthropicRequest(
            anthropic_version=self.anthropic_version,
            system=system,
            top_p=top_p,
            temperature=temperature,
            max_tokens=max_tokens,
            messages=[
                {
                    "role": msg.role.value,
                    "content": [{"type": "text", "text": msg.content}],
                }
                for msg in messages
            ],
        )

        async with self.session.client(
            service_name="bedrock-runtime",
            region_name=self.region_name,
        ) as client:
            try:
                response = await client.invoke_model_with_response_stream(
                    body=request.model_dump_json(exclude={"tools", "tool_choice"}),
                    modelId=self.model,
                    contentType="application/json",
                    accept="application/json",
                )
            except client.exceptions.ValidationException as e:
                raise ValueError(str(e))
            except botocore.exceptions.ClientError as e:
                raise InferenceException(str(e))

            out = ""

            async for chunk in response["body"]:
                chunk_json = json.loads(chunk["chunk"]["bytes"].decode())
                chunk_type = chunk_json["type"]

                if chunk_type == "content_block_delta":
                    content_type = chunk_json["delta"]["type"]
                    text = (
                        chunk_json["delta"]["text"]
                        if content_type == "text_delta"
                        else ""
                    )
                    out += text
                    yield text
                elif chunk_type == "message_start":
                    self._cost.input_tokens += chunk_json["message"]["usage"][
                        "input_tokens"
                    ]
                elif chunk_type == "message_delta":
                    self._cost.output_tokens += chunk_json["usage"]["output_tokens"]

            await self._trace(request.messages, [{"text": out}])

    @tracer.start_as_current_span(name="BedrockInferenceClient._tool_chain_stream")
    async def _tool_chain_stream(
        self,
        serialized_messages: List[Dict[str, Any]],
        tools: List[Tuple[Callable, Dict[str, Any]]],
        system: Optional[str] = None,
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        tool_choice="any",
        middlewares: List[Callable[[dict[str, Any]], Awaitable[None]]] = [],
    ):
        async with self.session.client(
            service_name="bedrock-runtime",
            region_name=self.region_name,
        ) as client:
            request = {
                "anthropic_version": self.anthropic_version,
                "top_p": top_p,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "messages": serialized_messages,
                "tools": [x[1] for x in tools],
                "tool_choice": {"type": tool_choice},
            }
            if system:
                request["system"] = [
                    {
                        "type": "text",
                        "text": system,
                        "cache_control": {"type": "ephemeral"},
                    }
                ]

            try:
                response = await client.invoke_model_with_response_stream(
                    body=json.dumps(request),
                    modelId=self.model,
                    contentType="application/json",
                    accept="application/json",
                )
            except client.exceptions.ValidationException as e:
                raise ValueError(str(e))
            except botocore.exceptions.ClientError as e:
                raise InferenceException(str(e))

            current_content = []
            current_block: dict[str, Any] = {
                "type": None,
            }
            current_json = ""

            async for chunk in response["body"]:
                chunk_json = json.loads(chunk["chunk"]["bytes"].decode())
                chunk_type = chunk_json["type"]

                if chunk_type == "message_start":
                    self._cost.input_tokens += chunk_json["message"]["usage"][
                        "input_tokens"
                    ]
                    self._cost.cache_read_input_tokens += chunk_json["message"][
                        "usage"
                    ].get("cache_read_input_tokens", 0)
                    self._cost.cache_write_input_tokens += chunk_json["message"][
                        "usage"
                    ].get("cache_creation_input_tokens", 0)
                elif chunk_type == "content_block_start":
                    block_type = chunk_json["content_block"]["type"]
                    if block_type == "text":
                        current_block = {
                            "type": "text",
                            "text": "",
                        }
                    elif block_type == "tool_use":
                        current_block = {
                            "type": "tool_use",
                            "id": chunk_json["content_block"]["id"],
                            "name": chunk_json["content_block"]["name"],
                            "input": {},
                        }
                        current_json = ""
                    elif block_type == "thinking":
                        current_block = {
                            "type": "thinking",
                            "thinking": chunk_json["content_block"]["thinking"],
                        }
                    elif block_type == "redacted_thinking":
                        current_block = chunk_json["content_block"]
                elif chunk_type == "content_block_delta":
                    if chunk_json["delta"]["type"] == "text_delta":
                        current_block["text"] += chunk_json["delta"]["text"]
                        for middleware in middlewares:
                            await middleware(current_block)
                    elif chunk_json["delta"]["type"] == "input_json_delta":
                        current_json += chunk_json["delta"]["partial_json"]
                        try:
                            current_block["input"] = pydantic_core.from_json(
                                current_json, allow_partial=True
                            )
                            for middleware in middlewares:
                                await middleware(current_block)
                        except ValueError:
                            pass
                    elif chunk_json["delta"]["type"] == "thinking_delta":
                        current_block["thinking"] += chunk_json["delta"]["thinking"]
                    elif chunk_json["delta"]["type"] == "signature_delta":
                        current_block["signature"] = chunk_json["delta"]["signature"]
                elif chunk_type == "content_block_stop":
                    current_content.append(current_block)

                    current_block = {
                        "type": None,
                    }
                elif chunk_type == "message_delta":
                    if chunk_json["delta"].get("stop_reason") == "tool_use":
                        self._cost.output_tokens += chunk_json["usage"]["output_tokens"]
                        break
                elif chunk_type == "error":
                    if chunk_json["error"]["type"] == "invalid_request_error":
                        raise ValueError(chunk_json["error"]["type"])
                    raise InferenceException(chunk_json["error"]["type"])
                elif chunk_type == "ping":
                    pass

            return current_content

        raise Exception("Max retries exceeded")


class AnthropicInferenceClient(InferenceClient):
    def __init__(
        self,
        model: str,
        api_key: str,
        api_url: str = "https://api.anthropic.com/v1/messages",
        thinking: Optional[int] = None,
    ):
        super().__init__()
        self.model = model
        self.api_url = api_url
        self.api_key = api_key
        self.thinking = thinking

    async def _post(self, request: dict):
        async with httpx.AsyncClient() as client:
            return await client.post(
                self.api_url,
                timeout=300000,
                json=request,
                headers={
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "anthropic-beta": "output-128k-2025-02-19",
                },
            )

    def _stream(self, client, request: dict):
        return client.stream(
            "POST",
            self.api_url,
            timeout=300000,
            json=request,
            headers={
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
                "anthropic-beta": "output-128k-2025-02-19,interleaved-thinking-2025-05-14",
            },
        )

    @tracer.start_as_current_span(name="AnthropicInferenceClient.get_generation")
    @backoff.on_exception(backoff.expo, InferenceException, max_time=60)
    async def get_generation(
        self, messages: List[ChatMessage], max_tokens=1024, top_p=0.9, temperature=0.5
    ):
        system = None
        if messages[0].role == ChatRole.SYSTEM:
            system = {
                "system": [
                    {"type": "text", "text": messages[0].content}
                    | (
                        {"cache_control": {"type": "ephemeral"}}
                        if messages[0].cache_marker
                        else {}
                    )
                ]
            }
            messages = messages[1:]

        request = {
            "model": self.model,
            "top_p": top_p,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "messages": [
                {"role": msg.role.value, "content": msg.content}
                | ({"cache_control": {"type": "ephemeral"}} if msg.cache_marker else {})
                for msg in messages
            ],
            "stream": False,
        }

        if system:
            request.update(system)

        if self.thinking:
            request["thinking"] = {"type": "enabled", "budget_tokens": self.thinking}
            request["max_tokens"] += self.thinking
            request["temperature"] = 1
            del request["top_p"]

        response = await self._post(
            request,
        )

        if response.status_code == 400:
            # TODO: ContextLengthExceeded
            raise ValueError(await response.aread())
        elif response.status_code != 200:
            raise InferenceException(await response.aread())

        body: dict = response.json()

        # https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#tracking-cache-performance
        self._cost.input_tokens += body["usage"]["input_tokens"]
        self._cost.cache_read_input_tokens += body["usage"].get(
            "cache_read_input_tokens", 0
        )
        self._cost.cache_write_input_tokens += body["usage"].get(
            "cache_creation_input_tokens", 0
        )
        self._cost.output_tokens += body["usage"]["output_tokens"]

        await self._trace(request["messages"], body["content"])

        return next(block["text"] for block in body["content"] if "text" in block)

    @tracer.start_as_current_span(name="AnthropicInferenceClient.connect_and_listen")
    async def connect_and_listen(
        self, messages: List[ChatMessage], max_tokens=1024, top_p=0.9, temperature=0.5
    ):

        system = None
        if messages[0].role == ChatRole.SYSTEM:
            system = {
                "system": [
                    {"type": "text", "text": messages[0].content}
                    | (
                        {"cache_control": {"type": "ephemeral"}}
                        if messages[0].cache_marker
                        else {}
                    )
                ]
            }
            messages = messages[1:]

        request = {
            "model": self.model,
            "top_p": top_p,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "messages": [
                {
                    "role": msg.role.value,
                    "content": [
                        {"type": "text", "text": msg.content}
                        | (
                            {"cache_control": {"type": "ephemeral"}}
                            if msg.cache_marker
                            else {}
                        )
                    ],
                }
                for msg in messages
            ],
            "stream": True,
        }

        if system:
            request.update(system)

        if self.thinking:
            request["thinking"] = {"type": "enabled", "budget_tokens": self.thinking}
            request["max_tokens"] += self.thinking
            request["temperature"] = 1
            del request["top_p"]

        async with httpx.AsyncClient() as client:
            async with self._stream(
                client,
                request,
            ) as response:
                if response.status_code == 400:
                    raise ValueError(await response.aread())
                elif response.status_code != 200:
                    raise InferenceException(await response.aread())

                out = ""

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        chunk_json = json.loads(line[6:])
                        chunk_type = chunk_json["type"]

                        if chunk_type == "content_block_delta":
                            content_type = chunk_json["delta"]["type"]
                            text = (
                                chunk_json["delta"]["text"]
                                if content_type == "text_delta"
                                else ""
                            )
                            out += text
                            yield text
                        elif chunk_type == "message_start":
                            self._cost.input_tokens += chunk_json["message"]["usage"][
                                "input_tokens"
                            ]
                            self._cost.cache_read_input_tokens += chunk_json["message"][
                                "usage"
                            ].get("cache_read_input_tokens", 0)
                            self._cost.cache_write_input_tokens += chunk_json[
                                "message"
                            ]["usage"].get("cache_creation_input_tokens", 0)
                        elif chunk_type == "message_delta":
                            self._cost.output_tokens += chunk_json["usage"][
                                "output_tokens"
                            ]

                await self._trace(request["messages"], [{"text": out}])

    @tracer.start_as_current_span(name="AnthropicInferenceClient._tool_chain_stream")
    async def _tool_chain_stream(
        self,
        serialized_messages,
        tools,
        system=None,
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        tool_choice="any",
        middlewares=[],
    ):
        request = {
            "model": self.model,
            "top_p": top_p,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "messages": serialized_messages,
            "stream": True,
            "tools": [x[1] for x in tools],
            "tool_choice": {"type": tool_choice},
        }
        if system:
            request["system"] = [
                {
                    "type": "text",
                    "text": system,
                    "cache_control": {"type": "ephemeral"},
                }
            ]

        if self.thinking:
            request["thinking"] = {"type": "enabled", "budget_tokens": self.thinking}
            request["max_tokens"] += self.thinking
            request["temperature"] = 1
            del request["top_p"]
            request["tool_choice"] = {"type": "auto"}

        current_content = []
        current_block: dict[str, Any] = {
            "type": None,
        }
        current_json = ""

        async with httpx.AsyncClient() as client:
            async with self._stream(
                client,
                request,
            ) as response:
                if response.status_code == 400:
                    raise ValueError(await response.aread())
                elif response.status_code == 413:
                    raise ContextLengthExceeded(await response.aread())
                elif response.status_code != 200:
                    raise InferenceException(await response.aread())

                async for line in response.aiter_lines():
                    if not line.startswith("data: "):
                        continue

                    chunk_json = json.loads(line[6:])
                    chunk_type = chunk_json["type"]

                    if chunk_type == "message_start":
                        self._cost.input_tokens += chunk_json["message"]["usage"][
                            "input_tokens"
                        ]
                        self._cost.cache_read_input_tokens += chunk_json["message"][
                            "usage"
                        ].get("cache_read_input_tokens", 0)
                        self._cost.cache_write_input_tokens += chunk_json["message"][
                            "usage"
                        ].get("cache_creation_input_tokens", 0)
                    elif chunk_type == "content_block_start":
                        block_type = chunk_json["content_block"]["type"]
                        if block_type == "text":
                            current_block = {
                                "type": "text",
                                "text": "",
                            }
                        elif block_type == "tool_use":
                            current_block = {
                                "type": "tool_use",
                                "id": chunk_json["content_block"]["id"],
                                "name": chunk_json["content_block"]["name"],
                                "input": {},
                            }
                            current_json = ""
                        elif block_type == "thinking":
                            current_block = {
                                "type": "thinking",
                                "thinking": chunk_json["content_block"]["thinking"],
                            }
                        elif block_type == "redacted_thinking":
                            current_block = chunk_json["content_block"]
                    elif chunk_type == "content_block_delta":
                        if chunk_json["delta"]["type"] == "text_delta":
                            current_block["text"] += chunk_json["delta"]["text"]
                            for middleware in middlewares:
                                await middleware(current_block)
                        elif chunk_json["delta"]["type"] == "input_json_delta":
                            current_json += chunk_json["delta"]["partial_json"]
                            try:
                                current_block["input"] = pydantic_core.from_json(
                                    current_json, allow_partial=True
                                )
                                for middleware in middlewares:
                                    await middleware(current_block)
                            except ValueError:
                                pass
                        elif chunk_json["delta"]["type"] == "thinking_delta":
                            current_block["thinking"] += chunk_json["delta"]["thinking"]
                        elif chunk_json["delta"]["type"] == "signature_delta":
                            current_block["signature"] = chunk_json["delta"][
                                "signature"
                            ]
                    elif chunk_type == "content_block_stop":
                        if current_block["type"]:
                            current_content.append(current_block)

                        current_block = {
                            "type": None,
                        }
                    elif chunk_type == "message_delta":
                        if chunk_json["delta"].get("stop_reason") == "tool_use":
                            self._cost.output_tokens += chunk_json["usage"][
                                "output_tokens"
                            ]
                            break
                    elif chunk_type == "error":
                        if chunk_json["error"]["type"] == "invalid_request_error":
                            raise ValueError(chunk_json["error"]["type"])
                        raise InferenceException(chunk_json["error"]["type"])
                    elif chunk_type == "ping":
                        pass
                    else:
                        logger.warning("Unknown message type from Anthropic stream.")

                return current_content


def _proto_to_dict(obj):
    type_name = str(type(obj).__name__)
    if hasattr(obj, "DESCRIPTOR"):  # Is protobuf message
        return {
            field.name: _proto_to_dict(getattr(obj, field.name))
            for field in obj.DESCRIPTOR.fields
        }
    elif type_name in ("RepeatedComposite", "RepeatedScalarContainer", "list", "tuple"):
        return [_proto_to_dict(x) for x in obj]
    elif type_name in ("dict", "MapComposite", "MessageMap"):
        return {k: _proto_to_dict(v) for k, v in obj.items()}
    return obj


def smart_unescape_code(s: str) -> str:
    # Common patterns in double-escaped code
    replacements = {
        "\\n": "\n",  # Newlines
        '\\"': '"',  # Quotes
        "\\'": "'",  # Single quotes
        "\\\\": "\\",  # Actual backslashes
        "\\t": "\t",  # Tabs
    }

    result = s
    for escaped, unescaped in replacements.items():
        result = result.replace(escaped, unescaped)

    return result


class GoogleAnthropicInferenceClient(AnthropicInferenceClient):
    def __init__(
        self,
        model: str,
        region: str = "us-east5",
        thinking: Optional[int] = None,
    ):
        InferenceClient.__init__(self)
        self.model = model
        self.region = region
        self.thinking = thinking
        self._get_token()

    def _get_token(self):
        if not hasattr(self, "creds"):
            self.creds, self.project_id = google.auth.default(
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )

        if not self.creds.token or self.creds.expired:
            request = google.auth.transport.requests.Request()
            self.creds.refresh(request)

        return self.creds.token

    async def _post(self, request: dict):
        request.pop("model", None)
        request["anthropic_version"] = "vertex-2023-10-16"

        async with httpx.AsyncClient() as client:
            return await client.post(
                f"https://{self.region}-aiplatform.googleapis.com/v1/projects/{self.project_id}/locations/{self.region}/publishers/anthropic/models/{self.model}:streamRawPredict",
                timeout=180,
                json=request,
                headers={
                    "Authorization": f"Bearer {self._get_token()}",
                },
            )

    def _stream(self, client, request: dict):
        request.pop("model", None)
        request["anthropic_version"] = "vertex-2023-10-16"

        return client.stream(
            "POST",
            f"https://{self.region}-aiplatform.googleapis.com/v1/projects/{self.project_id}/locations/{self.region}/publishers/anthropic/models/{self.model}:streamRawPredict",
            timeout=180,
            json=request,
            headers={
                "Authorization": f"Bearer {self._get_token()}",
            },
        )


class GoogleGenAIInferenceClient(InferenceClient):
    def __init__(
        self,
        model: str,
        api_key: Optional[str] = None,
        **kwargs,
    ):
        super().__init__()
        if api_key:
            self.client = genai.Client(api_key=api_key)
        else:
            self.client = genai.Client(**kwargs)
        self.model = model

    @tracer.start_as_current_span(name="GoogleGenAIInferenceClient._tool_chain_stream")
    async def _tool_chain_stream(
        self,
        serialized_messages,
        tools,
        max_tokens=1024,
        top_p=0.9,
        system=None,
        temperature=0.5,
        # Google is kind of meh with 'any' support
        tool_choice="any",
        middlewares=[],
    ):
        def convert_type_keys(schema):
            if not isinstance(schema, dict):
                return schema

            converted = {}
            for key, value in schema.items():
                if key == "type":
                    # Convert type value to uppercase and change key to type_
                    converted["type"] = value.upper()
                elif key == "properties":
                    # Recursively convert nested property schemas
                    converted[key] = {k: convert_type_keys(v) for k, v in value.items()}
                else:
                    # Recursively convert any other nested dictionaries
                    converted[key] = (
                        convert_type_keys(value) if isinstance(value, dict) else value
                    )

            if schema.get("type") == "OBJECT" and not schema.get("required"):
                schema["required"] = []

            return converted

        # Main tool processing
        filtered_tools = []
        for tool in tools:
            tool_schema = tool[1].copy()
            if "cache_control" in tool_schema:
                del tool_schema["cache_control"]

            # Convert the parameters schema
            parameters = tool_schema.get("parameters") or tool_schema.get(
                "input_schema"
            )

            if parameters:
                parameters = convert_type_keys(
                    tool_schema.get("parameters", tool_schema["input_schema"])
                )

            function_declaration = {
                "name": tool_schema["name"],
                "description": tool_schema["description"],
            }

            # vertexai mode doesn't support tools with no params (400s)
            if not parameters:
                parameters = {
                    "type": "OBJECT",
                    "properties": {},
                    "required": [],
                }

            function_declaration["parameters"] = parameters

            filtered_tools.append((tool[0], function_declaration))

        # Process messages to match Google's format
        processed_messages = []
        for i, message in enumerate(serialized_messages):
            assert isinstance(message["content"], list)
            parts = []
            for part in message["content"]:
                if part["type"] == "text":
                    parts.append(types.Part(text=part["text"]))
                elif part["type"] == "tool_use":
                    parts.append(
                        types.Part(
                            function_call=types.FunctionCall(
                                id=part["id"],
                                name=part["name"],
                                args=part["input"],
                            )
                        )
                    )
                elif part["type"] == "tool_result":
                    parts.append(
                        types.Part(
                            function_response=types.FunctionResponse(
                                id=part["tool_use_id"],
                                name=next(p2 for p2 in serialized_messages[i-1]["content"] if p2["type"] == "tool_use")["name"],
                                response={"result": part["content"]}, # docs conflict on whether to use "result" or "output" here
                            )
                        )
                    )

                processed_message = types.Content(
                    role=message["role"],
                    parts=parts,
                )

            processed_messages.append(processed_message)

        tool_config = {"function_calling_config": {"mode": tool_choice.upper()}}

        resp = await self.client.aio.models.generate_content_stream(
            model=self.model,
            contents=processed_messages,
            config=types.GenerateContentConfig(
                temperature=temperature,
                system_instruction=system,
                top_p=top_p,
                max_output_tokens=max_tokens,
                tool_config=tool_config,
                tools=[
                    types.Tool(function_declarations=[t[1]]) for t in filtered_tools
                ],
                thinking_config=types.ThinkingConfig(include_thoughts=True),
            ),
        )

        try:
            text_block = None
            tool_call_blocks = []

            async for chunk in resp:
                if chunk.candidates:
                    for part in chunk.candidates[0].content.parts:
                        if function_call := part.function_call:
                            args_dict = _proto_to_dict(function_call.args)
                            # Clean up over-escaped strings
                            if isinstance(args_dict, dict):
                                for k, v in args_dict.items():
                                    if isinstance(v, str):
                                        args_dict[k] = smart_unescape_code(v)

                            tool_block = {
                                "type": "tool_use",
                                "id": str(len(serialized_messages)) + "-" + str(len(tool_call_blocks)),
                                "name": function_call.name,
                                "input": args_dict,
                            }
                            tool_call_blocks.append(tool_block)
                            for middleware in middlewares:
                                await middleware(tool_block)
                        elif part.text:
                            if not text_block:
                                text_block = {"type": "text", "text": part.text}
                            else:
                                text_block["text"] += part.text

            return ([text_block] if text_block else []) + tool_call_blocks

        except Exception as e:
            raise InferenceException(str(e))

    @tracer.start_as_current_span(name="GoogleGenAIInferenceClient.get_generation")
    @backoff.on_exception(backoff.expo, InferenceException, max_time=60)
    async def get_generation(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=0.9, temperature=0.5
    ):
        system_instruction = None

        if messages[0].role.value == "system":
            system_instruction = messages[0].content
            messages = messages[1:]
        # Convert messages to Google's format
        processed_messages = [
            types.Content(role=msg.role.value, parts=[types.Part(text=msg.content)])
            for msg in messages
        ]

        response = await self.client.aio.models.generate_content(
            model=self.model,
            contents=processed_messages,
            config=types.GenerateContentConfig(
                temperature=temperature,
                system_instruction=system_instruction,
                top_p=top_p,
                max_output_tokens=max_tokens,
            ),
        )

        if not response.candidates:
            return ""

        await self._trace(
            [
                {
                    "role": msg.role.value,
                    "content": [{"type": "text", "text": msg.content}],
                }
                for msg in messages
            ],
            [{"text": response.text}],
        )

        return response.text

    @tracer.start_as_current_span(name="GoogleGenAIInferenceClient.connect_and_listen")
    async def connect_and_listen(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=0.9, temperature=0.5
    ):
        system_instruction = None

        if messages[0].role.value == "system":
            system_instruction = messages[0].content
            messages = messages[1:]

        processed_messages = [
            types.Content(role=msg.role.value, parts=[types.Part(text=msg.content)])
            for msg in messages
        ]

        response = await self.client.aio.models.generate_content_stream(
            model=self.model,
            contents=processed_messages,
            config=types.GenerateContentConfig(
                temperature=temperature,
                system_instruction=system_instruction,
                top_p=top_p,
                max_output_tokens=max_tokens,
            ),
        )

        out = ""

        async for chunk in response:
            if chunk.text:
                out += chunk.text
                yield chunk.text

        await self._trace(
            [
                {
                    "role": msg.role.value,
                    "content": [{"type": "text", "text": msg.content}],
                }
                for msg in messages
            ],
            [{"text": out}],
        )


class OAIRequest(AsimovBase):
    model: str
    messages: List[Dict[str, Any]]
    max_tokens: int = 4096
    temperature: float = 0.5
    top_p: float = 0.9
    stream: bool = False
    stream_options: Dict[str, Any] = {}
    tools: list[dict] = [{}]
    tool_choice: str = "none"


class OAIInferenceClient(InferenceClient):
    def __init__(
        self,
        model: str,
        api_key: str,
        api_url: str = "https://api.openai.com/v1/chat/completions",
    ):
        super().__init__()
        self.model = model
        self.api_url = api_url
        self.api_key = api_key

    @tracer.start_as_current_span(name="OAIInferenceClient.get_generation")
    @backoff.on_exception(backoff.expo, InferenceException, max_time=60)
    async def get_generation(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=1.0, temperature=0.5
    ):
        if self.model in ["o1-preview", "o1-mini"]:
            request = {
                "model": self.model,
                "messages": messages,
            }
        else:
            request = OAIRequest(
                model=self.model,
                messages=[
                    {
                        "role": m.role.value,
                        "content": m.content,
                    }
                    for m in messages
                ],
                max_tokens=max_tokens,
                top_p=top_p,
                temperature=temperature,
                stream=False,
            ).model_dump(exclude={"stream_options", "tools", "tool_choice"})

        if "o3" in self.model or "o4" in self.model:
            del request["temperature"]
            del request["top_p"]
            del request["max_tokens"]
            request["reasoning_effort"] = "high"

        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.api_url,
                json=request,
                timeout=180,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
            )

            if response.status_code == 400:
                raise ValueError(await response.aread())
            elif response.status_code != 200:
                raise InferenceException(await response.aread())

            body: dict = response.json()
            if body.get("error"):
                raise InferenceException(
                    body["error"]["message"] + f" ({body['error']})"
                )

            try:
                self._cost.input_tokens += body["usage"]["prompt_tokens"]
                self._cost.cache_read_input_tokens += (
                    body["usage"]
                    .get("prompt_tokens_details", {})
                    .get("cached_tokens", 0)
                )
                self._cost.output_tokens += body["usage"]["completion_tokens"]
            except KeyError:
                logger.warning(f"Malformed usage? {repr(body)}")

            await self._trace(
                request["messages"],
                [{"text": body["choices"][0]["message"]["content"]}],
            )

            return body["choices"][0]["message"]["content"]

    @tracer.start_as_current_span(name="OAIInferenceClient.connect_and_listen")
    async def connect_and_listen(
        self, messages: List[ChatMessage], max_tokens=4096, top_p=1.0, temperature=0.5
    ):
        request = OAIRequest(
            model=self.model,
            messages=[
                {
                    "role": msg.role.value,
                    "content": msg.content,
                }
                for msg in messages
            ],
            max_tokens=max_tokens,
            top_p=top_p,
            temperature=temperature,
            stream=True,
            stream_options={"include_usage": True},
        ).model_dump(exclude={"tools", "tool_choice"})

        if "o3" in self.model or "o4" in self.model:
            del request["temperature"]
            del request["top_p"]
            del request["max_tokens"]
            request["reasoning_effort"] = "high"

        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                self.api_url,
                json=request,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                timeout=180,
            ) as response:
                if response.status_code == 400:
                    raise ValueError(await response.aread())
                elif response.status_code != 200:
                    raise InferenceException(await response.aread())

                out = ""
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        if line.strip() == "data: [DONE]":
                            break
                        data = json.loads(line[6:])
                        if "error" in data:
                            raise InferenceException(
                                data["error"]["message"] + f" ({data['error']})"
                            )
                        if data["choices"] and data["choices"][0]["delta"].get(
                            "content"
                        ):
                            out += data["choices"][0]["delta"]["content"]
                            yield data["choices"][0]["delta"]["content"]
                        elif data.get("usage"):
                            self._cost.input_tokens += data["usage"]["prompt_tokens"]
                            # No reference to cached tokens in the docs for the streaming API response objects...
                            try:
                                self._cost.cache_read_input_tokens += (
                                    data["usage"]
                                    .get("prompt_tokens_details", {})
                                    .get("cached_tokens", 0)
                                )
                                self._cost.output_tokens += data["usage"][
                                    "completion_tokens"
                                ]
                            except AttributeError as e:
                                logger.warning(f"Malformed usage? {repr(data)}")

                await self._trace(
                    request["messages"],
                    [{"text": out}],
                )

    @property
    def include_cache_control(self):
        return "anthropic" in self.model

    @tracer.start_as_current_span(name="OAIInferenceClient._tool_chain_stream")
    async def _tool_chain_stream(
        self,
        serialized_messages,
        tools,
        system=None,
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        tool_choice="any",
        middlewares=[],
    ):
        if system:
            serialized_messages = [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": system,
                            "cache_control": {"type": "ephemeral"},
                        }
                    ],
                }
            ] + serialized_messages

        def wrap_tool_schema(tool):
            if "cache_control" in tool[1]:
                del tool[1]["cache_control"]

            return (tool[0], {"type": "function", "function": tool[1]})

        tools = list(map(wrap_tool_schema, tools))

        processed_messages = []

        for message in serialized_messages:
            if (not self.include_cache_control) and isinstance(
                message["content"], list
            ):
                for blk in message["content"]:
                    blk.pop("cache_control", None)

            if (
                message["role"] == "user"
                and isinstance(message["content"], list)
                and message["content"][0]["type"] == "tool_result"
            ):
                processed_messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": message["content"][0]["tool_use_id"],
                        "content": message["content"][0]["content"],
                    }
                )
            else:
                tc = next(
                    (
                        content
                        for content in message["content"]
                        if content["type"] == "tool_use"
                    ),
                    None,
                )
                if tc:
                    # OpenRouter hangs if there is message content here?
                    message = {
                        "role": message["role"],
                        "tool_calls": [
                            {
                                "id": tc["id"],
                                "type": "function",
                                "function": {
                                    "name": tc["name"],
                                    "arguments": json.dumps(tc["input"]),
                                },
                            }
                        ],
                    }
                elif type(message["content"]) == list:
                    message = {
                        "role": message["role"],
                        "content": message["content"][0]["text"],
                    }
                processed_messages.append(message)

        request = OAIRequest(
            model=self.model,
            messages=processed_messages,
            max_tokens=max_tokens,
            top_p=top_p,
            temperature=temperature,
            stream=True,
            stream_options={"include_usage": True},
            tools=[x[1] for x in tools],
            tool_choice=tool_choice,
        )

        request = request.__dict__

        if "o3" in self.model or "o4" in self.model:
            del request["temperature"]
            del request["top_p"]
            del request["max_tokens"]
            request["reasoning_effort"] = "high"
            request["tool_choice"] = "required"

        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                self.api_url,
                timeout=300000,
                json=request,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
            ) as response:
                if response.status_code == 400:
                    raise ValueError(await response.aread())
                elif response.status_code != 200:
                    raise InferenceException(await response.aread())

                text_block = None
                tool_call_blocks = []

                async for line in response.aiter_lines():
                    if line == "data: [DONE]":
                        return ([text_block] if text_block else []) + tool_call_blocks
                    if line.startswith("data: "):
                        data = json.loads(line[6:])
                        if data["choices"]:
                            d = data["choices"][0]["delta"]
                            if d.get("content"):
                                if not text_block:
                                    text_block = {
                                        "type": "text",
                                        "text": d["content"],
                                    }
                                else:
                                    text_block["text"] += d["content"]
                            for tc in d.get("tool_calls", []):
                                if "id" in tc:
                                    tool_call_blocks.append(
                                        {
                                            "type": "tool_use",
                                            "id": tc["id"],
                                            "name": tc["function"]["name"],
                                            "raw_input": "",
                                            "input": {},
                                        }
                                    )
                                tool_call_blocks[tc["index"]]["raw_input"] += tc[
                                    "function"
                                ]["arguments"]
                                try:
                                    tool_call_blocks[tc["index"]]["input"] = (
                                        pydantic_core.from_json(
                                            tool_call_blocks[tc["index"]]["raw_input"],
                                            allow_partial=True,
                                        )
                                    )
                                    for middleware in middlewares:
                                        await middleware(tool_call_blocks[tc["index"]])
                                except ValueError:
                                    pass
                        if data.get("usage"):
                            self._cost.input_tokens += data["usage"]["prompt_tokens"]
                            self._cost.output_tokens += data["usage"][
                                "completion_tokens"
                            ]


class OpenRouterInferenceClient(OAIInferenceClient):
    def __init__(self, model: str, api_key: str):
        super().__init__(
            model,
            api_key,
            api_url="https://openrouter.ai/api/v1/chat/completions",
        )

    async def _populate_cost(self, id: str):
        await asyncio.sleep(0.5)
        async with httpx.AsyncClient() as client:
            for _ in range(3):
                response = await client.get(
                    f"https://openrouter.ai/api/v1/generation?id={id}",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                )

                if response.status_code == 404:
                    await asyncio.sleep(0.5)
                    continue

                if response.status_code != 200:
                    await asyncio.sleep(0.5)
                    continue

                body: dict = response.json()

                self._cost.input_tokens += body["data"]["native_tokens_prompt"]
                self._cost.output_tokens += body["data"]["native_tokens_completion"]
                self._cost.dollar_adjust += -(body["data"]["cache_discount"] or 0)
                return

    @tracer.start_as_current_span(name="OpenRouterInferenceClient._tool_chain_stream")
    async def _tool_chain_stream(
        self,
        serialized_messages,
        tools,
        system=None,
        max_tokens=1024,
        top_p=0.9,
        temperature=0.5,
        tool_choice="any",
        middlewares=[],
    ):
        if system:
            serialized_messages = [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": system,
                            "cache_control": {"type": "ephemeral"},
                        },
                    ],
                }
            ] + serialized_messages

        openrouter_messages = []
        for message in serialized_messages:
            if (not self.include_cache_control) and isinstance(
                message["content"], list
            ):
                for blk in message["content"]:
                    blk.pop("cache_control", None)

            if (
                message["role"] == "user"
                and isinstance(message["content"], list)
                and message["content"][0]["type"] == "tool_result"
            ):
                openrouter_messages.extend(
                    {
                        "role": "tool",
                        "tool_call_id": result["tool_use_id"],
                        # This has to be raw string not a {"type": "text", "text": ...} dict
                        "content": result["content"],
                    }
                    for result in message["content"]
                )
                openrouter_messages.append(
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Here is the result of the tool call",  # need text here to enable caching since cache control can only be set on text blocks.
                            }
                            | (
                                {"cache_control": {"type": "ephemeral"}}
                                if "cache_control" in message["content"][-1]
                                else {}
                            )
                        ],
                    }
                )
            else:
                tool_calls = [
                    content
                    for content in message["content"]
                    if content["type"] == "tool_use"
                ]
                if tool_calls:
                    text = next(
                        (
                            content
                            for content in message["content"]
                            if content["type"] == "text"
                        ),
                        None,
                    )
                    message = {
                        "role": "assistant",
                        "content": text,
                        "tool_calls": [
                            {
                                "id": tc["id"],
                                "type": "function",
                                "function": {
                                    "name": tc["name"],
                                    "arguments": json.dumps(tc["input"]),
                                },
                            }
                            for tc in tool_calls
                        ],
                    }
                openrouter_messages.append(message.copy())

        openrouter_tools = []
        for _, tool in tools:
            openrouter_tools.append(
                {
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description"),
                        "parameters": tool["input_schema"],
                    },
                }
            )

        request = {
            "model": self.model,
            "messages": openrouter_messages,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "temperature": temperature,
            "stream": True,
            "tool_choice": tool_choice,
            "tools": openrouter_tools,
        }

        if self.model in ["openai/o3-mini"]:
            request["reasoning_effort"] = "high"

        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                self.api_url,
                json=request,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                timeout=60,
            ) as response:
                if response.status_code == 400:
                    raise ValueError(await response.aread())
                elif response.status_code != 200:
                    raise InferenceException(await response.aread())

                id = None
                text_block = None
                tool_call_blocks = []

                async for line in response.aiter_lines():
                    if line == "data: [DONE]":
                        await self._populate_cost(id)
                        return ([text_block] if text_block else []) + tool_call_blocks
                    if line.startswith("data: "):
                        data = json.loads(line[6:])
                        if "id" in data:
                            id = data["id"]
                        if data.get("error"):
                            if "context" in str(data["error"]):
                                raise ContextLengthExceeded(str(data["error"]))
                            elif "invalid_request_error" in str(data["error"]):
                                raise NonRetryableException(str(data["error"]))
                            raise InferenceException(
                                data["error"]["message"] + f" ({data['error']})"
                            )
                        if data["choices"]:
                            d = data["choices"][0]["delta"]
                            if d.get("content"):
                                if not text_block:
                                    text_block = {
                                        "type": "text",
                                        "text": d["content"],
                                    }
                                else:
                                    text_block["text"] += d["content"]
                            for tc in d.get("tool_calls", []):
                                if tc.get("id"):
                                    tool_call_blocks.append(
                                        {
                                            "type": "tool_use",
                                            "id": tc["id"],
                                            "name": tc["function"]["name"],
                                            "raw_input": "",
                                            "input": {},
                                        }
                                    )
                                tool_call_blocks[tc["index"]]["raw_input"] += tc[
                                    "function"
                                ].get("arguments", "")
                                try:
                                    tool_call_blocks[tc["index"]]["input"] = (
                                        pydantic_core.from_json(
                                            tool_call_blocks[tc["index"]]["raw_input"],
                                            allow_partial=True,
                                        )
                                    )
                                    for middleware in middlewares:
                                        await middleware(tool_call_blocks[tc["index"]])
                                except ValueError:
                                    pass
